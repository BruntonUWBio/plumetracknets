\documentclass[5p,twocolumn,authoryear]{elsarticle}

\usepackage{amsmath}            
\usepackage{epstopdf}           
\usepackage{flushend}    % Dunno why       

\usepackage{caption,subcaption}
\usepackage{graphicx}
\usepackage{pgffor}

\usepackage{xcolor}		% colors for hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = magenta,
    urlcolor  = blue,
    citecolor = purple,
    anchorcolor = black,
}   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{multirow}
\usepackage[normalem]{ulem}


%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

% For reviwer response
% https://tex.stackexchange.com/questions/515094/how-to-automatically-reference-line-numbers-of-a-quote
% https://tex.stackexchange.com/questions/2193/how-to-reference-the-page-of-a-figure
\usepackage{refcount}
\usepackage{ifthen}
\newcommand{\linerange}[2]{%
\ifthenelse{\equal{\getrefnumber{#1}}{\getrefnumber{#2}}}{%
line \ref{#1}%
}{%
lines \ref{#1}--\ref{#2}%
}%
}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage[switch]{lineno} % left+right hand side
\linenumbers


%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
% \biboptions{comma,round}
\biboptions{numbers,square,sort&compress}
% \usepackage[numbers,square,sort&compress]{natbib}


% if you have landscape tables
\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

\usepackage{subfigure}

% declarations for front matter

\setlength{\emergencystretch}{3em} % Avoid text overflow into margins

\usepackage{custom_macros}
\renewcommand{\top}{\mathsf{T}}
\newcommand{\bLam}{\boldsymbol{\Lambda}}
\newcommand{\bell}{\boldsymbol{\ell}}
\newcommand{\bzero}{\boldsymbol{0}}




\begin{document}



\begin{frontmatter}

%%%%%%%%%%%%%%%%   Title   %%%%%%%%%%%%%%%%
\title{
Emergent behavior and neural dynamics \\
in artificial agents tracking turbulent plumes
}

\author[ECE]{Satpreet H. Singh\corref{cor1}}
\ead{satsingh@uw.edu}
\cortext[cor1]{Author for correspondence}
\author[UNR]{Floris van Breugel}
\author[CSE,ECE,CNT]{Rajesh P. N. Rao}
\author[Bio,eScience]{Bingni W. Brunton}

\address[ECE]{Department of Electrical and Computer Engineering, University of Washington, Seattle, USA.}
\address[UNR]{Department of Mechanical Engineering, University of Nevada, Reno, USA.}
\address[CSE]{Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA.}
\address[CNT]{Center for Neurotechnology, University of Washington, Seattle, USA.}
\address[Bio]{Department of Biology, University of Washington, Seattle, USA.}
\address[eScience]{eScience Institute, University of Washington, Seattle, USA.}



\begin{abstract}
\linenumbers
\begin{linenumbers}
Tracking a turbulent plume to locate its source under variable wind and plume statistics is a complex task; flying insects routinely accomplish such tracking, often over long distances, in pursuit of food or mates.
Several aspects of this remarkable behavior and its underlying neural circuitry have been studied experimentally. 
Here, we take a complementary \emph{in silico} approach to develop an integrated understanding of behavior and neural computations.
Specifically, we train artificial recurrent neural network (RNN) agents using deep reinforcement learning (DRL) to locate the source of simulated turbulent plumes.
Interestingly, the agents' emergent behaviors resemble those of flying insects, and the RNNs learn to compute task-relevant variables with distinct dynamic structures in population activity.
Our analyses put forward a testable behavioral hypothesis for tracking plumes in changing wind direction, and we provide key intuitions for memory requirements and neural dynamics in turbulent plume tracking.
\end{linenumbers}
\end{abstract}

\begin{keyword}
deep reinforcement learning \sep 
olfactory search \sep
plume tracking \sep
recurrent neural networks \sep
computational neuroscience \sep 
% control theory 
\end{keyword}
\end{frontmatter}



\section*{Introduction}
Locating the source of an odor in a windy environment is a challenging control problem, where an agent must act to correct course in the face of intermittent odor signals, changing wind directions, and the variability in odor plume shape \citep{reddyannrev,celani2014odor}.
Moreover, an agent tracking an intermittent plume needs memory, where current and past egocentric 
odor, visual, and wind sensory signals must be integrated to determine the next action.
For flying insects, localizing the source of odor plumes emanating from potential food sources or mates is critical for survival and reproduction.
Therefore, many aspects of their plume tracking abilities have been experimentally studied in great detail \citep{baker2018algorithms, park2016neurally, carde2008navigational,currier2020multisensory}.
However, most such studies are limited to one or two levels of analysis such as behavior \citep{van2008insects}, computation \citep{lochmatter2009theoretical,pang2018history} or neural implementation \citep{sun2018analysis}.

Despite the wide adoption of wind tunnel experiments to study odor plume tracking \citep{van2014plume}, generating controlled dynamic turbulent plumes and recording flight trajectories at high resolution is expensive and laborious.
Exciting alternative approaches have been developed using virtual reality \citep{kaushik2020characterizing}
and kilometer-scale outdoor dispersal experiments \citep{leitch2020long}. 
While behavioral experiments are now tractable, collecting significant neural data during free flight in small insects remains technologically infeasible, and larger insects require larger wind tunnels.
Here we are motivated to take a complementary \textit{in silico} approach using artificial recurrent neural network (RNN) agents trained to track simulated turbulent plumes, with the goal of developing an integrated understanding of the behavioral strategies and the associated neural computations that support plume tracking.

%%%%%%%%%%% Fig 1 %%%%%%%%%%%%%
\begin{figure*}[h!]
\centering
\includegraphics[width=1.0\linewidth]{fig_training.pdf}
\caption{
\textbf{Training artificial agents to track turbulent plumes with deep reinforcement learning.}
    \textbf{(a)} 
    A schematic of a flying insect performing a plume tracking task, showing \textit{upwind surges}, \textit{crosswind casts}, and \textit{U-turns} behaviors (inspired by a figure in \cite{baker2018algorithms}).
    In this work, we model the spatial scale (dashed rectangle) where the insect can use only olfactory and mechanosensory wind sensing cues for plume tracking.
    \textbf{(b)} 
    The plume simulator models stochastic emission of odor packets from a source carried by wind. 
    Odor packets are subject to advection by wind, random cross-wind perturbation, and radial diffusion.
    \textbf{(c)}
    An example of a plume simulation where the wind direction changed several times.
    The centerline of the plume is in red.
    \textbf{(d)} 
    A schematic of how the artificial agent interacts with the environment at each time step. 
    The plume simulator model of the environment determines the sensory information available to the agent $\mathbf{x}$ (egocentric wind direction vector and local odor concentration) and the rewards used in training.
    The agent navigates within the environment with actions $\mathbf{a}$ (turn direction and magnitude of movement).
    \textbf{(e)} Agents are modeled as neural networks and trained by deep reinforcement learning (DRL). 
    A recurrent neural network (RNN) generates an internal state representation $\mathbf{h}$ from sensory observations, followed by parallel Actor and Critic heads that implement the agent's control policy and predict the state values, respectively.
    The Actor and Critic heads are 2-layer, feedforward multi-layer perceptron (MLP) networks.
    \textbf{(f)} 
    A schematic to illustrate an agent's head-direction, course-direction, and the wind direction, all measured with respect to the ground and counter-clockwise from the x-axis.
    Course direction is the direction that the agent actually moves in, accounting for the effect of the wind on the agent's intended direction of movement (head-direction).
    Egocentric wind direction is the direction of the wind as sensed by the agent. 
    }
\label{fig_training}
\end{figure*}


In recent years, artificial neural networks (ANNs) have gained increasing popularity for modeling and understanding aspects of neural function and animal behavior \citep{kietzmann2019deep, cichy2019deep}, including vision \citep{kriegeskorte2015deep}, movement \citep{sussillo2015neural}, navigation   \citep{kanitscheider2017training, cueva2018emergence, cueva2019emergence, haesemeyer2019convergent}, and collective behaviors \citep{verma2018efficient}. 
Whereas many ANNs have been trained using supervised approaches that rely on labeled training data, an alternative emerging algorithmic toolkit known as deep reinforcement learning (DRL) has made it computationally tractable to train ANN agents (Figure \ref{fig_training}d).
In particular, an ANN agent receives sensory observations and task-aligned rewards based on its actions at each step and tries to learn a strategy for its next actions to maximize total expected reward  \citep{arulkumaran2017deep,sutton2018reinforcement}.
Such learning and optimization based models are \textit{normative} in the sense that they can prescribe how a neural system \textit{should} behave, rather than describing how it has been observed to behave.
As neuroscience moves towards studying increasingly naturalistic behaviors \citep{nastase2020keep,sonkusare2019naturalistic,huk_beyond_2018,GOMEZMARIN201925}, such normative approaches are gaining traction as tools to gain insight, rapidly explore hypotheses, and generate ideas for theoretical development  \citep{richards2019deep,le2020towards,merel2019deep,ahrens2019zebrafish,banino2018vector,colabrese2017flow,verma2018efficient}. 


Flying insects search for sources of odor using several strategies, depending on the spatial scale being considered and odor source visibility \citep{baker2018algorithms} (Figure \ref{fig_training}a). 
Close to the odor source, insects can fly to the source guided by vision.
At longer ranges (from several meters up to about $100$ meters \citep{wall1987range}) or when the odor source is not yet visible, their search must be guided by olfaction to detect odors and mechanosensation to estimate wind velocity.
At this larger scale, there are a few stereotyped behavioral sequences known to be important for plume tracking: 
\textit{upwind surges} when the insect can sense the odor, and 
\textit{crosswind casts} and \textit{U-turns} to locate the plume body when the insect loses the odor scent (Figure~\ref{fig_training}a) \citep{carde2008navigational}.
Here we focus on this larger-scale odor and wind guided regime, where agents have access to only mechanosensory and olfactory cues.

%%%%%%%%%%% Fig 2 %%%%%%%%%%%%%%%%%% 
\begin{figure*}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig_qual.pdf}
\caption{
\textbf{Examples of successful plume tracking trajectories and associated odor sensory streams under various plume simulator configurations}.  
Left column shows snapshots of odor plumes (grey)  overlaid with RNN agent trajectories, which are colored by whether the agent was able to sense the presence (green) or absence (dark blue) of odor. 
% Importantly, because the plume simulation models stochastic odor packets, the agent encounters odors only intermittently.
Trajectories start at filled black circle and end at the odor source, indicated by dotted cross-hairs in the lower left of each sub-figure.
The plume visualizations are from the end of the tracking episode (last frame) and thus deviates from the plume as observed by the agent during the episode.
The arrow within dotted circle above the cross-hair shows direction of wind at time of snapshot.
All examples use a 0.5 m/s wind. 
Middle column shows odor concentration profiles at vertical breadth-wise grid lines in the simulated arena, $x=\{2, 4, 6, 8\}$m.
Right column plots show odor concentration as sensed by the agent over time $C(t)$, and odor concentration profiles along the horizontal length-wise grid line at $y=0$m.
Each row is a different plume configuration, and they are are:
\textbf{(a)} `constant' left-to-right wind direction plume. 
\textbf{(b)} `sparse' plume with same left-to-right constant wind direction but reduced (0.4x) birthrate of odor packets.
\textbf{(c)} `sparser' plume, which is like the `sparse' configuration and additionally has a reduced (0.5x) puff radial diffusion-rate.
\textbf{(d)} `switch-many' plume with wind direction switches occurring every $\sim 3s$ 
\textbf{(e)} `switch-once' plume, which makes one 45$^{\circ}$ counter-clockwise wind direction switch during the tracking episode.
Supplementary animations provide additional examples of successful and unsuccessful tracking episodes.
% \url{https://github.com/BruntonUWBio/plumetracknets}.
}
\label{fig_behavior_qual}
\end{center}
\end{figure*}


In this paper, we describe behaviors that emerge in RNN agents trained to track odors in a flexible plume simulation and analyze the neural dynamics that underlie these behaviors.
At a behavioral level, we find that the agents' actions can be summarized by  modules that closely resemble those observed in flying insects. 
% (Section \ref{sec_behavior_qual}).
While odor plumes that do not change in direction can be tracked using a few steps of history, longer timescales of memory are essential for plumes that are non-stationary and change directions unpredictably.
Interestingly, the learned tracking behavior of RNN agents in non-stationary plumes suggests a testable experimental hypothesis:  that tracking is accomplished through local plume shape rather than wind direction.
% (Section \ref{sec_centerline}).
The RNNs learn to represent variables known to be important to flying insect navigation, such as head direction and time between odor encounters.
% (Section \ref{sec_repr}).
Further, the low-dimensional neural activity associated with the emergent behavior modules represents behaviorally relevant variables and is structured into two distinct regimes.
% (Section \ref{sec_dynamics}).
% , and transitions between these regimes are asymmetric in duration (Section \ref{sec_ttcs}).

%%%%%%%%%%%% Fig 3 %%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig_regimes_coursedir.pdf} 
% \includegraphics[width=1.0\linewidth]{fig_regimes.pdf} 
% \includegraphics[width=1.0\linewidth]{fig_coursedir.pdf} 
\caption{
\textbf{Emergent plume tracking behavior can be decomposed into distinct behavior modules.}
\textbf{(a \& b)} 
Trajectories for a successful \textbf{(a)} and an unsuccessful \textbf{(b)}
plume tracking episode showing three distinct behavior modules: \textit{tracking} (green), \textit{lost} (red) and \textit{recovering} (purple-blue)
\textbf{(c)}
Kernel density estimates show data aggregated from an equal number of successful and unsuccessful constant wind direction plume tracking episodes (N timesteps, E episodes). 
Plots reveal differences between the three behavior modules across key behavioral measures:
\textit{Head-direction}: 
Head-direction densities are concentrated around $\pm 180^{\circ}$, a signature of zig-zagging but mostly upwind movement; 
the concentration of density around the upwind direction reduces from tracking to recover to lost, accounting for the more complex trajectories encountered in the latter two behavior modules.
Angles are measured counterclockwise with 0$^{\circ}$ indicating directly downwind.
\textit{$\mathbf{\Delta x}$ and $\mathbf{\Delta y}$}: Density estimates for drift in the x-direction ($\Delta x$) and y-direction ($\Delta y$) per timestep show how tracking is characterized by primarily upwind (negative x-direction) movement in both \textit{tracking} and \textit{recover} modules, but lesser so in the \textit{lost} module.
Y-direction movements are significant in the \textit{tracking} and \textit{recovering} modules, corresponding to more complex turning behaviors, but minimal in the \textit{lost} module.
\textit{Turn action}: Left/right turning movements are balanced in the \textit{tracking} module as the agent closely tracks the edge of the plume, but is biased towards clockwise movements in the other two modules, especially the \textit{lost} module.
\textit{Move action}: The agent significantly modulates its forward movement speed in the \textit{lost} module only.
\textit{Stray distance}: The agent strays from the plume minimally in the \textit{tracking} module, but significantly otherwise.   
\textbf{Empirical distributions of course direction suggest that agents track plume with respect to plume centerline rather than current wind direction.}
\textbf{(d--f)} show kernel density estimates of an agent's course direction (CD) relative to the local plume centerline (solid blue) and to the current wind direction (dashed orange) in three plume configurations.
$\pm180^{\circ}$ means anti-parallel movement with respect to the plume centerline, or exactly upwind movement with respect to the wind direction.
\textbf{(d)}
For the constant wind direction plume, both the course direction distributions are equivalent.
\textbf{(e)}
In the `switch-once' configuration, a significant proportion of time is spent at a $\approx45^{\circ}$ angle to the wind, but is actually aligned (anti-parallel) with the plume centerline.
\textbf{(f)}
In the `switch-many' configuration, once again,
a significant proportion of time is spent at an angle to the upwind direction, but course direction is actually aligned with the plume centerline.
% See \ref{sec_supp_centerline} for corresponding plots for all 5 RNN agents.
Bottom row subfigure titles indicate how many timesteps (N) and how many successful episodes (E) were summarized in each plot.
Plots show data for RNN Agent 3. 
See Supplementary Information for corresponding plots for other agents.
}
\label{fig_regimes_coursedir}
\end{center}
\end{figure*}


%% ------------------------------------------------------- %%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \clearpage
% \section*{Behavior and neural dynamics of trained agents}
\section*{Results}
\label{sec_analysis}
Our \emph{in silico} agents learn strategies to successfully localize plume sources in turbulent, non-stationary environments.
In this section, we briefly summarize our approach, characterize agent performance, then highlight their emergent behavioral and neural features.
In addition to comparing artificial agents to biology, we discover behavioral strategies that motivate future experiments and gain intuition about the neural computations underlying these emergent behaviors.

\subsection*{Training artificial agents to track turbulent plumes}

We use a particle-based 2D plume model \citep{farrell2002filament}, which is computationally tractable and approximates the statistics of real-world turbulent plumes (Figure \ref{fig_training}b).
Agents are actor-critic neural networks \citep{konda2000actor} that receive continuous valued sensory observations as inputs (i.e. egocentric instantaneous wind velocity and local odor concentration) and produce continuous valued move and turn actions (Figure \ref{fig_training}e).
Parameters of the environment simulation and agent actions are roughly matched to the capability of flies.
Training is done using the Proximal Policy Gradient (PPO) \cite{schulman2017proximal} algorithm, with agents initialized at random locations within or slightly outside plumes that switch directions multiple times during the course of the episode.  
% Our reward function primarily rewards homing in on the odor source and secondarily, rewards actions that reduce distance to the odor source.

For evaluation, we assess trained agents on additional simulations across four wind configurations: 
\textit{`constant'}, where the wind direction is held constant ($0^{\circ}$) throughout the episode; 
\textit{`switch-once'}, where the wind makes one 45$^{\circ}$ counter-clockwise switch during the episode; 
\textit{`switch-many'}, where the wind direction changes at multiple random times during the episode; 
\textit{`sparse'}, which is the same as the `constant' configuration except that the puff birth-rate is reduced (0.4x), resulting in odor distributions more representative of those seen in turbulent flow.
To demonstrate that our agents still perform well when odors are highly intermittent, we also include additional simulations on \textit{`sparser'} plumes, in which the puff radial diffusion-rate are lowered (0.5x) in addition to lowering the puff birth-rate as is done in `sparse' plumes.
Unless otherwise specified, we describe results from one agent chosen at random from among the top-5 performing of 14 trained agents.
Analyses of the other 4 agents are in the Extended Data  and Supplement.
See Methods for more details.

\subsection*{Agents track plumes with varying wind conditions using distinct behavioral modules}
\label{sec_behavior_qual}

Our trained RNN agents are able to complete the plume tracking task with changing wind direction and varying plume sparsity
(Figure \ref{fig_behavior_qual} shows some example trajectories). 
The observed trajectories can be summarized by three behavior modules, determined approximately by the time elapsed since the agent last sensed odor (Figure \ref{fig_regimes_coursedir}). 
We refer to these three modules as \textit{tracking}, \textit{lost}, and \textit{recovering}. 
In the \textit{tracking} module, the agent rapidly moves closer to the plume source, using either straight-line trajectories when it is well within the plume, or a quasi-periodic `plume skimming' behavior where it stays close to the edge of the plume while moving in and out of it. 
The interval between the agent's encounters with odor packets in this module is under 0.5 seconds.
\textit{Recovering} corresponds to an irregular behavior where the agent makes large, usually cross-wind, movements after having lost track of the plume for a relatively short period of time (about 0.5 second).
\textit{Lost} corresponds to a periodic behavior that appears variably across trained agents as either a spiraling or slithering/oscillating motion, often with an additional slow drift in an arbitrary direction.
This behavior is seen when the agent has not encountered the plume for a relatively long time, typically over 1 second.
Thresholds used to segment each agent's trajectories into behavior modules were determined by visual inspection (See Extended Data Table \ref{table_supp_module_threshold}). 

%%%%%%%%%%%% Fig 4 %%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig_repr.pdf} 
\caption{
\textbf{Neural activity of RNN is low-dimensional and represents biologically relevant variables.}
(a--d) 
Neural activity trajectories plotted over a diversity of plume conditions and tracking outcomes, 
\textbf{(a)} colored by agent head-direction $\Theta_{HEAD}$, 
\textbf{(b)} steps since last odor encounter $T_{last}$, 
\textbf{(c)} exponentially-weighted moving-average of odor concentration ($odor_{EWMA}$, window-size = 8 steps), and
\textbf{(d)} exponentially-weighted moving-average of recent odor encounters ($odor_{ENC}$, window-size = 46 steps).
\textbf{(e)}
Quality of fit ($R^2$) of a linear model regressing neural activity onto $odor_{EWMA}$ and $odor_{ENC}$ for sliding-windows of varying lengths. 
The sliding-window size for subfigures \textbf{(c)} and \textbf{(d)} are determined by identifying the peaks of these curves. 
\textbf{(e)} 
Plot of cumulative variance explained by top principal components of neural activity aggregated across multiple plume configurations (`constant', 'switch-once' \& `switch-many') suggests a low-dimensional structure. 
\textbf{(f)}
Horizontal box-plots of feature permutation importance scores of classifier trained to predict agent actions.
Features include quantities plotted in subfigures (a--d) 
($\Theta_{HEAD}$, $T_{last}$, $odor_{EWMA}$, and $odor_{ENC}$), 
and instantaneous egocentric sensory observations (wind $w_X, w_Y$ and odor).
\textbf{(g)} 
90$\%$ of the variance of the 64-dimensional neural activity can be explained by the first-5 principal components.
% See \ref{sec_supp_module} for corresponding plots for all 5 RNN agents.
See Supplementary Information for corresponding plots for other agents.
}
\label{fig_representations}
\end{center}
\end{figure*}

Agents that are successful in tracking plumes in constant wind direction primarily use the \textit{tracking} and \textit{recovering} modules (see supplementary animations).
Successful trajectories on the `switch-once' and `switch-many' plumes reveal that RNN agents use more complex strategies in the face of changing wind directions.
If an agent is in the \textit{tracking} module and well within the plume at the time of wind direction change, it continues along its path until it reaches the edge of the plume without changing its actions.
If it is skimming the edge of the plume when the wind direction switch happens, it tries to compensate for the added movement of the plume by making more pronounced oscillations in and out of the plume.
% The shape of these oscillations appears to depend more on the local shape of the plume than on the current direction of the wind (explored further in Section \ref{sec_centerline}). 
Finally, if the agent cannot keep up with the movement of the plume, it typically orchestrates a sequence of large oscillations and spiral-like movements, corresponding to the \textit{recovering} and \textit{lost} modules, to try to find the plume boundary.
On returning to the plume, it resumes the \textit{tracking} module behaviors once again.

Agents are able to execute successful tracking in sparse plumes, even when the odor encounters are increasingly intermittent (example trajectories in Figure~\ref{fig_behavior_qual}b-c). 
In these examples, we decreased the birth rate and diffusion-rate of the odor packets in the plume simulation (Figure~\ref{fig_training}b), resulting in environments with the cross-wind odor profiles are strongly non-Gaussian and with even sparser odor encounters by the agent.

%%%%%%%%%%% Fig 6 %%%%%%%%%%%%%%%%%% 
\begin{figure*}[th!]
\begin{center}
    \includegraphics[width=1.0\linewidth]{fig_dynamics.pdf}
\caption{
\textbf{Neural dynamics appear to organize themselves into overlapping yet distinct regimes.}
\textbf{(a)} Plume tracking episode that ends in successful homing-in on the odor source, and 
\textbf{(b)} Unsuccessful episode that strays from the plume and ends up exceeding the simulator's bounds. 
\textbf{(c \& d)} Neural activity plots corresponding to each row's trajectory projected on a 2D subspace (state-space) generated from the first 2 principal components of that episode's neural activity.  
Quiver arrows correspond to direction of neural activity gradient, and are colored by the agent's current behavior module.
\textbf{(c)} A `funnel' like structure (in green) emerges in the state-space corresponding to the \textit{tracking} behavior module. 
\textbf{(d)} The agent's periodic \textit{lost} behavior shows up as a limit-cycle in the state-space (red). 
\textbf{(e)} Neural activity plotted over multiple trajectories comprising a diversity of plume conditions and tracking outcomes, projected onto the first 3 principal components of the aggregated neural activity and colored by behavior module.
Examples from RNN agent 3.
% See  \ref{sec_supp_dynamics} for corresponding plots for all 5 RNN agents.
See Supplementary Information for corresponding plots for other agents.
}
\label{fig_dynamics}
\end{center}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\subsection*{Agents track plume centerline, not current wind direction}
\label{sec_centerline}

Successful trajectories in plumes that switch direction suggest that agents take the local shape of the plume into account, rather than just the current wind conditions (Figure \ref{fig_regimes_coursedir}e--f and supplementary animations).
To quantify this, we look at the empirical distributions of an agent's course direction computed with respect to the current wind direction, and with respect to the centerline of the nearby plume (Figure \ref{fig_training}c).
The agent's course direction (Figure \ref{fig_training}f) is defined as the direction of its instantaneous movement with respect to the ground. 
(See Methods for details on calculations.)
Figure \ref{fig_regimes_coursedir} shows that the empirical course direction distributions are much better aligned with the plume centerline than to the wind for one example agent.
For `switch-once' plumes, the peak of the course direction distribution is much closer to $\pm180^{\circ}$ when considered relative to the centerline than relative to the wind direction.
This observation indicates that the agent's flight is on average aligned (anti-parallel) with the plume centerline, but at an $\approx 45^{\circ}$ angle with respect to the current wind direction.
Similarly, the same trend holds in the `switch-many' configuration, where the course direction distribution is aligned with the plume centerline, but diverges from the wind direction.
% In \ref{sec_supp_centerline}, we see that this trend holds across all 5 RNN agents.
This trend holds across all 5 RNN agents (Supplementary Information).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \clearpage
\subsection*{Neural activity is low-dimensional and represents task-relevant variables}
\label{sec_repr}

We now turn our attention to the neural dynamics of the RNNs as agents perform plume tracking.  
Rather than characterizing the activity of individual units, we consider the population activity of the network \citep{ebitz2021population,saxena2019towards}.

First, we reduce and visualize the population activity of our RNN across the `constant', 'switch-once' \& `switch-many' plume configurations and find that the neural activity is low-dimensional (Figure \ref{fig_representations}g), with the first 5--8 principal components explaining 90$\%$ of the variance in the 64-dimensional population activity. 
% In \ref{sec_supp_repr}, 
This trend holds across all 5 RNN agents (Supplementary Information).

To gain insights into the computations supporting the plume tracking behavior, we look for variables represented in this low-dimensional population activity that are relevant for solving the task. 
We find that the RNNs have learned to represent task-relevant quantities beyond the instantaneous egocentric sensory observations received from the simulator (Figure \ref{fig_representations}a--d).

Interestingly, these quantities reflect information necessary for solving these challenging plume tracking tasks and require memories of past sensory cues encountered by the agent.
First, the agent's \textit{head-direction}, or its the orientation with respect to the ground, is evident in Figure~\ref{fig_representations}a.
The \textit{time since plume was last encountered} is encoded as in Figure~\ref{fig_representations}b and may be involved in determining transitions between behavior modules.
Whereas the agent only receives local odor concentrations as a sensory input, we find that an exponentially-weighted moving-average of sensed \textit{odor concentrations} is present in Figure~\ref{fig_representations}c.
We conjecture this quantity may be useful as a memory in the face of an intermittent odor signal arising from a turbulent plume.
Similarly, an exponentially-weighted moving-average of a discretized \textit{odor encounters} signal is evident in Figure~\ref{fig_representations}d.



To quantify how important these represented variables are to actual task performance, we train a Random Forest (RF) \citep{breiman2001random} classifier to predict the (discretized) actions taken by the agent over successful trajectories 
(see Methods for details).
% (see \ref{sec_supp_repr} for details).
We also estimate the relative importance of each input feature by calculating its permutation importance score \citep{strobl2008conditional, breiman2001random}, which is an estimate of the reduction in the classifierâ€™s accuracy across several (N=30) randomized permutations of that feature.
Classifier accuracies using all aforementioned represented features (Figure \ref{fig_representations}f) along with instantaneous egocentric sensory features is 10--18\% higher across all agents than that using classifiers receiving just instantaneous egocentric sensory observations, and 26--51$\%$ higher across all agents than that produced by a majority-class classifier 
(See Extended Data Tables \ref{table_supp_ewma} and \ref{table_supp_repr} for each agent's feature metadata and classifier accuracies respectively). 
Represented variables have permutation importance scores within the range covered by the importance scores of the instantaneous egocentric sensory inputs.
\textit{Time since plume was last encountered} is always one of the top two most important features, close to the x-component of the egocentric wind velocity.
The two time-averaged odor features always easily dwarf the importance of the instantaneous odor feature.
Furthermore, time-averaged \textit{odor concentrations} are more important than time-averaged \textit{odor encounters} in 4 out of 5 agents.
\textit{Head-direction} has an importance intermediate to the two time-averaged odor features in 4 out of 5 agents.
Note that the estimates provided by this analysis are approximate due to the discretization of the action data and correlations between features.
% See \ref{sec_supp_repr} for results at the individual agent level for all 5 agents.

%%%%%%%%%%%% Figure Connectivity/Arch %%%%%%%%%%%%%%%%%
\begin{figure*}[htbp!]
\begin{center}
\includegraphics[width=0.95\linewidth]{fig_eigen_mlps.pdf}
% \includegraphics[width=1.0\linewidth]{home_by_arch_facet.png}
\caption{
 \textbf{Plume tracking requires memory, especially when wind changes direction.} 
\textbf{(a)} Eigenvalue spectra of the RNN recurrence matrix $\mathbf{W_{h}} \in \mathbb{R}^{64 \times 64}$ (for Agent 3) before and after training show how training results in the generation of unstable modes.
\textbf{(b)} Time-averaged (over 6 episodes and 1738 timesteps) stimulus integration timescales associated with stable eigenmodes of recurrence Jacobian $\mathbf{J}^{\mathrm{rec}}$ show a bulk of relatively short timescales (within $12$ timesteps).  
Top 5 integration timescales for the agent shown are 56.5, 13.0, 7.7, 6.8 \& 5.8 timesteps.
Before training, timescales associated with $\mathbf{W_{h}}$'s eigenmodes can be large, even exceeding the length of the training/evaluation episodes (300 steps or 12 seconds).
99\% confidence interval bands have been plotted for the after-training timescales curve, but these bands are of negligible magnitude and therefore invisible. 
% See  \ref{sec_supp_eigen} for corresponding plots and data for all RNN agents.
See Supplementary Information for corresponding plots for other agents.
\textbf{(c)--(f)}
% \textbf{Memory capacity improves plume tracking, especially in non-stationary wind direction plumes.}
Number of successful homing episodes for different agent architectures, across different plume configurations for the same set of 240 initial conditions across varying agent starting location and head direction, and plume simulator state.
`MLP\_X' refers to feedforward networks with X timesteps of sensory history. 
RNNs generally outperform feedforward networks, with more pronounced gains for more complex, switching wind direction (`switch-once', `switch-many') plume tasks.
In feedforward networks, performance on plumes with switching wind direction can improve significantly with increasing memory.
Regression lines (solid black) are fit on only MLP data, but are extended slightly (dotted line) for comparison with RNNs
(p-values are for a two-sided Wald Test with null hypothesis that the slope is zero).
}
\label{fig_eigen_mlps}
\end{center}
\end{figure*}


% #############################
% \clearpage
\subsection*{Neural dynamics organized into two structured regimes and a transition region}
\label{sec_dynamics}

We now examine the dynamics of the RNN activations (hidden state) and how it evolves over the course of tracking episodes.
This analysis is inspired by previous work characterizing the nonlinear dynamics of RNN agents by their fixed-points and transitions among them \citep{vyas2020computation, sussillo2013opening, maheswaranathan2019universality}.
However, in a noteworthy deviation from these structures, we did not find any fixed points in our RNNs. 
Instead, our RNNs adopt neural dynamics that are better described by dynamical regimes.
Specifically, the dynamics appear to organize themselves into overlapping but distinctly structures associated with the \textit{tracking} and \textit{lost} behavioral modules (Figure \ref{fig_dynamics}). 
Interestingly, the periodic spiral or oscillatory movements seen in the \textit{lost} behavioral module appear to also have a quasi-periodic limit-cycle structure in the neural state space (Figure \ref{fig_dynamics}d), while 
the neural dynamics associated with the \textit{tracking} behavior are represented as quasi-periodic `funnel' like structures (Figure \ref{fig_dynamics}c).
We also see an amorphous transition region associated with the \textit{recovering} behavioral module.
We see the same approximate structures (limit-cycles and funnel) emerge in the neural dynamics for 4 of the 5 RNN agents. 
% See \ref{sec_supp_dynamics} for data on all 5 agents. 
See Supplementary Information for data on all 5 agents. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \subsection*{Macroscopic transitions between neural activity regimes are asymmetric in duration}
% \label{sec_ttcs}

% After having found distinct neural activity regimes for the \textit{tracking} and \textit{lost} behaviors in the previous section, we now explore transitions between these two regimes.
% Specifically, we look at differences in the duration between 
% (1) when an agent enters the plume and when it `enters' the \textit{tracking} neural activity regime, and 
% (2) when an agent leaves the plume and when it `enters' the \textit{lost} neural activity regime.
% Entry into a neural activity regime is determined by when the neural activity is within a pre-specified distance from a `centroid' corresponding to that regime (see details in \ref{sec_supp_ttcs}).    
% As shown in Figure \ref{fig_ttcs}, the time taken to enter the \textit{lost} neural activity regime after the agent leaves the plume is significantly longer than the time taken to enter the \textit{tracking} neural activity regime after the agent enters the plume.
% In \ref{sec_supp_ttcs}, we see that this trend holds across 4 out of 5 agents. 


%#############################
\subsection*{Connectivity of trained RNNs reveal signatures of instability and memory}
\label{sec_eigen}

The weight matrices and recurrence Jacobians of our RNNs after training offer some theoretical insights into how the neural dynamics of the artificial agents are shaped to track turbulent plumes.

We find that the training process reorganizes the eigenvalue spectrum of the RNN recurrence matrix $\bW_h$ (Figure~\ref{fig_eigen_mlps}a; also see Methods for definition).
Before training, weights are initialized as normally distributed random variables with associated eigenvalues randomly distributed within the unit circle.
After training, there are multiple eigenvalues outside the unit circle in the complex plane.
Interestingly, for all 5 agents, there is at least one strictly real-valued eigenvalue larger than unity.
Along with external stimuli, these unstable eigenvalues drive the network's hidden dynamics.
% \footnote{Animations showing how recurrence Jacobian eigenspectra change over the course of  tracking episodes: \url{https://github.com/BruntonUWBio/plumetracknets/blob/main/VRNN3-eigen.md}}.

Comparing the time-averaged stimulus integration timescales of trained RNNs (see Methods) with those of the untrained RNNs reveals that training adjusts these timescales to lie well within the maximum episode length of 300 timesteps (Figure~\ref{fig_eigen_mlps}b).
Furthermore, we see that the bulk of these timescales are within about $12$ timesteps ($\approx 0.5$s), suggesting that the plume tracking task predominantly needs short timescale memories.
In Extended Data Table \ref{table_supp_taus}, we see that this trend holds across all 5 RNNs. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \textbf{Recurrence and memory enable plume tracking}
% \label{sec_behavior_quant}

Finally, to understand the role of memory capacity in plume tracking, we compare the performance of our trained RNNs to trained feedforward multi-layer perceptron networks (MLPs) that receive varying timescales of sensory history (see Methods). 
As seen in Figure \ref{fig_eigen_mlps}c--f, RNNs outperform MLPs for every plume tracking task, with the performance gains being largest in the most challenging tasks.
For MLPs, longer duration sensory memories support much better performance on tougher tracking tasks, where the plumes switch more often or odor packets are sparser.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\section*{Discussion}
% \noindent \textbf{Connections to tracking turbulent plumes in biology:} 
Our artificial RNN agents exhibit similarities to biology at the levels of behavior, computation, and neural dynamics. 
In this section, we draw these comparisons, discuss their significance, and suggest theoretical insights that may be relevant for researchers interested in biological plume tracking.

\subsection*{Behavioral features}
The complex behavior exhibited by our agents can be decomposed into simpler modules, 
sequenced by the time elapsed since the agent last encountered the plume (Figure \ref{fig_regimes_coursedir}).
These modules show features similar to \textit{upwind surging}, \textit{crosswind casting} and \textit{U-turn} behaviors previously reported in many studies on moths, fruit flies, and other flying insects \citep{baker2018algorithms,van2014plume,carde2008navigational, budick2006free}.
The spiraling behavior seen in the agent's \textit{lost} behavior module has been previously proposed as a plume reacquisition strategy \citep{lochmatter2009theoretical}; however, it deviates from the gradually widening crosswind casting strategy typically seen in flying insects.
Furthermore, the variable sequencing behavior modules resembles the odor loss activated clock mechanism that has been previously proposed to drive changes in flight behavior in moths \citep{kennedy1974pheromone,kennedy1983zigzagging,baker1990upwind}.

Our observations make a behavioral hypothesis that agents track plumes with respect to the centerline rather than with respect to the current wind direction.
In a previous study on tracking in constant wind direction plumes, \cite{grunbaum2015spatial} proposed a model where insects explicitly performed upwind surges when close to the plume centerline.
However, a later study by \cite{pang2018history} failed to find support for this model.
Our analysis provides intuition for the role of centerline tracking in non-stationary plumes and suggests a testable hypothesis: we predict that centerline tracking behaviors will be more apparent in flying insects when they track plumes in wind that switches direction.

\subsection*{Algorithms for odor localization}
How biological organisms search and localize odor sources has a long and rich literature, and a variety of algorithms has been developed to explain this capability of single-celled organisms, cells in an organ, and animals in complex environments.
Where gradients exist, these smoothly varying rates of changes in concentration may be exploited to localize odor sources by chemotaxis and related algorithms \citep{adler1966chemotaxis,friedrich2007chemotaxis,cremer2019chemotaxis}.
However, in intermittent odor landscapes, gradient-based algorithms cannot be successful, and the \emph{Infotaxis} algorithm was developed as an alternative \citep{balkovsky2002olfactory,vergassola2007infotaxis,masson2009chasing,barbieri2011trajectories,calhoun2014maximally}. 

Both Infotaxis \citep{vergassola2007infotaxis} and our approach are formulated as solutions to plume tracking as a Partially Observable Markov Decision Process (POMDP) \citep{sutton2018reinforcement}. 
Infotaxis chooses actions (movements) to maximally reduce the expected entropy of the odor source location probability on the next time step.
This makes two computational requirements of the agent.
First, agents must store a probability distribution for the source location spanning the size of the arena being navigated.
Second, agents must perform Bayesian inference \citep{reddyannrev}.
In contrast, here our approach is to learn this control policy from only locally available measurements, and actions are chosen to maximize the expected discounted reward over a trajectory.
Compared to Infotaxis, our approach produces trajectories with a stronger semblance to biology and a control policy that reacts to changing wind conditions.
It also uses a neural implementation that does not make any (potentially biologically implausible) assumptions about which variables are implemented or how inference is performed.


\subsection*{Neural representations}
Our RNN agents learn to represent variables that have been previously reported to be crucial to odor navigation (Figure \ref{fig_representations}).
First, \textit{agent head-direction} has been found to be implemented as a ring attractor circuit in the central complex of many flying insects and is implicated in navigation \citep{pfeiffer2014organization,seelig2015neural,green2017neural,kim2019generation,okubo2020neural}.
Second, \textit{time since plume was last encountered}
is analogous to the hypothesized internal-clock that determines behavior switching in moths \citep{kennedy1974pheromone,kennedy1983zigzagging,baker1990upwind}.
Additionally, \cite{park2016neurally} showed how this variable is encoded by the bursting olfactory receptor neurons (bORNs) in many animals, and that it contains information relevant to navigating in turbulent odors.


Third, \textit{exponential moving-average of odor encounters} was found by \cite{demir2020walking} to determine the probability of turn and stop behaviors in walking flies navigating in turbulent plumes.
Specifically, higher odor encounter rates were associated with more frequent saccadic upwind turns
\citep{celani2020olfactory}.
Fourth, \textit{exponentially moving-average of sensed odor concentration}  
is motivated by theoretical work by \cite{maheswaranathan2019reverse} that posits exponentially-weighted moving averages to be good canonical models for stimulus integration in RNNs.
Between these two time-averaged odor variables, the best represented window length for time-averaged concentration is significantly shorter ($\approx0.3$s) than that for time-averaged encounters ($\approx 1.9$s). 
Furthermore, we find that time-averaged odor concentration is relatively better represented and more important 
in predicting agent behavior, 
corroborating the intuition that turn decisions during flight would require quick decision-making on sub-second timescales.
We note that alternative variables beyond these four may exist that better explain agent navigation decisions.

\subsection*{Neural dynamics}
The agents' neural activity is low dimensional and structured, with an interesting  asymmetry in macroscopic transitions between these structures.
Like often seen in neurobiological recordings \citep{pang2016dimensionality,cunningham2014dimensionality}, the population activity of our RNNs is low-dimensional, with the top 5--8 principal components explaining an overwhelming majority of the 64-dimensional population's total variance (Figure \ref{fig_representations}g).

The neural dynamics associated with behavior modules further exhibit interesting structure.
\textit{Lost} behaviors are represented as quasi-limit-cycles, while \textit{tracking} behaviors show a `funnel' like structure (Figure \ref{fig_dynamics}).
Similar 1-D circular manifolds and 2-D funnels \citep{vyas2020computation,kriegeskorte2021neural} have been previously reported on the representational geometry of sensory populations, but not, to the best of our knowledge, in the closed-loop agent setting.


% Finally, we find that the interval between entering the neural activity cluster associated with the \textit{lost} behavior and leaving the plume, is significantly longer than the interval between entering neural activity cluster associated with the \textit{tracking} behavior and entering the plume.
% This asymmetry in macro-scale transitions in the neural state resembles an asymmetry in behavior transitions reported in \cite{van2014plume}, where the authors experimentally observe that flies take about twice as long to cast crosswind after plume loss, than to surge upwind on encountering attractive odors.

\subsection*{The role of memory}
Two independent analyses give us insight into the memory requirements of the plume tracking task (Figure \ref{fig_eigen_mlps}).
We find that the bulk of stimulus integration timescales are within $\sim12$ steps or 0.5s, and that longer sensory histories and network recurrence lead to better performance on more challenging tasks, such when plumes switch direction.
Together, we believe that memory is crucial for tracking non-stationary wind direction plumes, but short timescale (under $\sim0.5s$) and reflexive mechanisms may be sufficient for tracking constant wind direction plumes.
This corroborates results by \cite{pang2018history} and \cite{grunbaum2015spatial} and extends them by highlighting the importance of longer term memory in cases where wind changes direction. 
\\

% \noindent \textbf{Limitations and future work}:
\subsection*{Limitations and future work}
Our results motivate several avenues of further development.
First, our plume simulator is a computationally efficient but only approximate model that can provide a sufficiently realistic time series of odor encounters for a moving agent.
However, it does not capture some aspects of real plumes, such as the the filamentous nature of plumes \citep{celani2014odor}, or the variation of whiff duration and whiff frequency as a function from source \citep{villermaux1999geometry}.
Further developments in efficient yet highly accurate models of turbulent flows \citep{stachenfeld2021learned} could provide better simulations where finer-timescale interactions between agents and simulations could be learned.

Second, here we used vanilla recurrent units with no biomechanical body model, and models that incorporate known complexity from biology as constraints may give rise to further insights.
For instance, DRL agents may be trained using spiking neural networks \citep{Adden2020,yuan2019reinforcement,recanatesi2019dimensionality}.
Further, the wealth of architectural insights emerging from the fly connectome may be used to constrain wiring motifs in artificial networks \citep{hulse2020connectome,scheffer2020connectome}.
Modeling multiple antennae \citep{Kadakia2021,reddy2021sector}, or more generally a biomechanical body, would enrich the interactions between the agent and the simulation environment \citep{rios2021neuromechfly,plum2021scant,merel2019deep}.

Third, multi-task training should produce agents with richer behaviors and more complex neural activity structures with shared and
task-specific adaptations \citep{crawshaw2020multi,yang2019task,duncker2020organizing,mlynarski2018adaptive,weber2019role}.
Adding other sensory modalities like vision and training the agents in a 3D virtual reality environment could produce more realistic perceptual representations in the agent \citep{crosby2019animal, crosby2020building}.

Finally, future work could explore learning algorithms that respect biological constraints like excitation-inhibition balance and Dale's law \citep{GOULAS2021,ehrlich2021psychrnn,delahunt2018biological}.
More complex training curricula \citep{bengio2009curriculum} or
alternative training algorithms using evolutionary techniques \citep{de2013evolutionary,stanley2019designing,gupta2021embodied} might be able to mitigate the significant performance variability we observed in our agents.


Our analyses also motivate further methodological development in theoretical tools to understand actor-critic RNNs.
Currently available reverse-engineering methods that characterize RNNs using discrete dynamical features such as fixed-points  \citep{sussillo2013opening,maheswaranathan2019universality,maheswaranathan2019reverse} are not applicable to the continuous and amorphous dynamical structures that we encountered in our analyses (Figure \ref{fig_dynamics}).
New methods are also needed for comparing multiple agents at the behavioral level, specifically taking into account the compounding differences that arise from small differences in action-stimulus loops.
Finally, further theoretical work is required to understand the role of training-induced unstable RNN connectivity eigenmodes, such as those observed in Figure~\ref{fig_eigen_mlps}, including extensions of analytic techniques developed to understand RNNs trained by supervised-learning \citep{sussillo2009generating,rajan2006eigenvalue,maheswaranathan2019reverse,mikulik2020meta,schaeffer2020reverse}.
\\

\subsection*{Conclusion} 
% \noindent \textbf{Conclusion:} 
In this paper, we used deep reinforcement learning to train recurrent neural network agents to solve a stochastic plume tracking task.
We find several behavioral and neural features that emerge in these trained agents and connect these features with how flying insects track turbulent plumes.
Our findings motivate future experiments and theoretical developments, and provide a foundation for more nuanced future work.
We hope our approach will contribute to the growing convergence in the understanding of artificial and biological networks \citep{hasson2020direct,hassabis2017neuroscience}.
Efforts to reverse engineer such neural network agents will help accelerate the development of similar methods for biological agents \citep{kwon2020inverse,ashwood2020inferring}.
Moreover, our RNN agents may serve as generative models of complex naturalistic behaviors, which may facilitate the development of behavior analysis tools for biology \citep{berman2016predictability,singh2021mining,nassar2018tree}.
Insights from these studies may also inspire the development of robotic agents with artificial \citep{vouloutsi2013synthetic} or hybrid \citep{anderson2019smellicopter} olfactory sensing.


%%%%%%%%%%%%%%s
\clearpage
\section*{Methods}

\subsection*{Plume simulation}
\label{sec_methods_plume}
We implement a particle-based two-dimensional plume simulation model (Figure \ref{fig_training}f) that replicates the statistics of real-world turbulent plumes \citep{farrell2002filament}.
This type of simulation has been used in a wide range of domains including olfactory navigation \citep{carde2008navigational}, robotics \citep{kowadlo2008robot} and sensor networks \citep{michaelides2005plume}. 
The simulator (Figure \ref{fig_training}b) comprises a spatially homogeneous wind vector-field (0.5 m/s with configurable direction) and an odor source located at the origin that emits odor puffs as a Poisson process.
Puffs are initialized with a fixed initial radius ($r_0$) and concentration ($c_{0}$).
They then undergo a fixed-rate radial diffusion ($r_{t} = r_{t-1} + r_{\delta} $) such that their concentration reduces in proportion to their increase in volume, i.e.
$ c_{t} = c_{0} \left( {r_{0}}/{r_{t}} \right)^3 $.
In addition, each emitted puff is advected downwind at the wind velocity and perturbed randomly by crosswind translation.
In other words, each puff effectively performs a biased random walk downwind over time, while diffusing in concentration spatially.
Our simulated plumes and agents are constrained to two dimensions for simplicity of analysis.
The dimensions of the simulated arena are $[-2m,+10m]$ and $[-5m, +5m]$ in the x and y axes respectively, totaling a $120m^2$ arena.
Plumes are simulated at 100 iterations/second. 
The plume's centerline is obtained by simulating puffs that have no random crosswind translation at each iteration (Figure \ref{fig_training}f).  

We simulate the following four wind configurations. 
First, the wind direction is held constant ($0^{\circ}$) throughout the simulation (`constant').
Second, the wind direction makes one 45$^{\circ}$ counter-clockwise switch during a tracking episode (`switch-once'). 
Third, the wind direction switches at multiple random times during a tracking episode (`switch-many').
Each wind direction turn is a random draw from a Gaussian distribution with mean $0$ and 45$^{\circ}$ s.d., truncated at $\pm 60^{\circ}$, and occurs approximately every 3 seconds.
Fourth, the wind direction is held constant, but the puff birth-rate is reduced (0.4x) compared to the 'constant' configuration (`sparse').
% See \ref{sec_supp_train_eval} for further details on the plume simulation.
See Supplementary Information for further details on the plume simulation.


%%%%%%%%%%%%% Fig 4 %%%%%%%%%%%%%%%%
% \begin{figure*}[h!]
% \centering
% \includegraphics[width=1.0\linewidth]{fig_coursedir.pdf} 
% \caption{
% \textbf{Empirical distributions of course direction suggest that agents track plume with respect to plume centerline rather than current wind direction.}
% Kernel density estimates of the agents' course direction (CD) are shown relative to the local plume centerline (solid blue) and to the current wind direction (dashed orange) in three plume configurations.
% $\pm180^{\circ}$ implies anti-parallel movement with respect to the plume centerline, or exactly upwind movement with respect to the wind direction.
% \textbf{(a)}
% For the constant wind direction plume, both the course direction distributions are equivalent.
% \textbf{(b)}
% In the `switch-once' configuration, a significant proportion of time is spent at a $\approx45^{\circ}$ angle to the wind, but is actually aligned (anti-parallel) with the plume centerline.
% \textbf{(c)}
% In the `switch-many' configuration, once again,
% a significant proportion of time is spent at an angle to the upwind direction, but course direction is actually aligned with the plume centerline.
% Plots show data for RNN Agent 3. 
% See \ref{sec_supp_centerline} for corresponding plots for all 5 RNN agents.
% Subfigure titles indicate how many timesteps (N) and how many successful episodes (E) were summarized in each plot.
% }
% \label{fig_regimes_coursedir}
% \end{figure*}

\subsection*{Agent architecture}
\label{sec_methods_arch}
Our agents are actor-critic networks (Figure \ref{fig_training}e), where a recurrent neural network (RNN) receives sensory observations and passes a transformed representation of them onto parallel Actor and Critic heads
that are both two-layer multi-layer perceptrons (MLPs) \citep{konda2000actor}.
The Actor head implements a control policy to map the RNN's learned state representation to actions, while the Critic head implements a value function that maps the state representation to an estimate of the state's value based on rewards.
This value function is used only during agent training and not thereafter.
In the DRL literature, two-layer deep heads are typically sufficiently expressive for such control problems \citep{hill2018stable}.
At each time step, an agent receives a 3-dimensional real-valued input vector comprising egocentric wind velocities $(x, y)$ and odor concentration at its current location.
In response, the agent produces continuous valued turn (maximum $\pm  6.25 \pi$ radians/s) and forward-movement (maximum 2.5 m/s) actions; these velocities are matched to the capabilities of flying fruit flies \citep{van2014plume,van2008insects}.
In contrast to the orthogonal initialization typically employed in the mainstream machine learning literature \citep{henaff2016recurrent}, we initialize our RNNs with normally distributed weights to facilitate comparisons with the computational neuroscience literature \citep{vogels2005neural,sussillo2014neural,yang2019task}. 

Additionally, to understand the role of memory on tracking performance, we compare the RNN-based agents with an alternative feedforward-only network (multi-layer perceptron, MLP) architecture with fixed-length memory (Figure \ref{fig_eigen_mlps}), simulated by appending historical sensory observations onto instantaneous network inputs \citep{mnih2013playing}.
Although such MLPs are far from being biologically plausible architectures, they serve as useful tools for abstract comparison since their memory capacities can be controlled precisely. 
Both RNN and MLP layers across all agents are 64 units wide with $tanh$ nonlinearities.


\subsection*{Agent training and evaluation}
\label{sec_methods_train_eval}
We train our agents using the Proximal Policy Gradient (PPO) algorithm  \citep{schulman2017proximal}, which is known to robustly solve continuous observation-space continuous action-space control problems without needing significant hyperparameter tuning.  
To guide agent training, we developed a curriculum and a simple reward function that greatly rewards homing in on odor source, mildly rewards actions that reduce the radial distance between agent and odor source, and penalizes longer duration trajectories and straying too far from the plume. 
We train 14 independently randomly initialized networks for each architecture type, i.e. RNNs and MLPs with 2, 4, 6, 8, 10 \& 12 timesteps of observation history.

Next, we evaluated each trained agent's performance with a behavioral assay. 
Each trained agent is evaluated with 240 episodes at different initializations (15 initial locations, 2 initial simulation timestamps, and 8 initial head-directions), and at each of the 'constant', 'switch-once' and `switch-many' plume configurations.
For each architecture type, we proceed to analyze only the top 5 seeds with the best performance, as measured by total number of successful episodes across the four plume configurations.
Agent training/evaluation episodes are run at $25$ frames per second on a sub-sampled plume and limited to 300 frames/timesteps (12 seconds of flight) per episode to accelerate DRL training.
% See \ref{sec_supp_train_eval} for additional details on agent training and evaluation, and \ref{sec_supp_hyperparams} for a full list of associated hyperparameters.
To demonstrate agent performance on more patchy and turbulent plumes, the simulations used for all presented analyses and figures use a plume radial diffusion-rate that is 50\% of the rate used while training. 
See Extended Data Tables \ref{table_supp_plume}--\ref{table_supp_training} for all associated hyperparameters, and Supplementary Information for for additional details on agent training and evaluation.

\subsection*{Agents track plume centerline, not current wind direction}
Subtracting the current wind direction angle from the course direction  provides the course direction with respect to the wind.
To find the course direction with respect to the centerline, we first find the median local centerline angle using centerline puffs (Figure \ref{fig_training}c) within a $\pm 2$ c.m. band of the x-coordinate of the agent's location, then subtract this from the course-direction with respect to the ground.
The empirical distributions include aggregate data from when agents are in the \textit{tracking} behavior module from up to 60 random successful trajectories from the `constant', `switch-once' and `switch-many' plume configurations.
Additionally, for the 'switch-once' configuration, we trim trajectories to consider only the timesteps after the wind direction switch has occurred. 


\subsection*{Neural activity dimensionality and neural representations}
% \label{sec_methods_repr}

\noindent \textit{Odor encounters}: Our definition of odor encounters is identical to that used in \cite{demir2020walking}. 
The stream of odor inputs is discretized to be 1 at the first timestep of the stream where the odor is perceptible and 0 for the remaining contiguous steps where it is still perceptible. 
% Other processing is as described in Section \ref{sec_repr}.

\noindent \textit{Agent action classifier}: To quantify how important these represented variables are to actual task performance, we train a Random Forest (RF) \citep{breiman2001random} classifier to predict actions taken by the agent over successful trajectories.
We uniformly partition the Turn and Move action variable, which are continuous valued, into domains of 3 and 2 discrete classes respectively.
These classes correspond roughly to `left', `center' and `right' turns, and to `fast' and `slow' forward movements.
These are concatenated to form a 6-class independent variable.
The classifier receives instantaneous sensory observations (egocentric wind speed x and y components $w_X, w_Y$ and odor concentration) and the four aforementioned encoded features as inputs. 
Training and test sets are a randomized non-overlapping 80$\%$--20$\%$ split of evaluation episodes, balanced across plume configuration and episode outcomes.
We do a 20-trial 3-fold cross-validated randomized search over the number-of-estimators (range: [10,50]) hyperparameter, and then train a classifier using the best hyperparameter on the whole training set. 
We next estimate the relative importance of each input feature by calculating its permutation importance score \citep{strobl2008conditional, breiman2001random}, which is an estimate of the reduction in the classifierâ€™s accuracy across several (N=30) randomized permutations of that feature.
Note again that the estimates provided by this analysis are approximate due to the discretization of the action data and correlations between features.

We determine the window-sizes \citep{ewma_pandas} for odor concentrations and encounters by linearly regressing neural activity onto them for sliding-windows of varying lengths, and we chose the window-size that produces the best fit as measured by the coefficient of determination $R^2$ (Figure \ref{fig_representations}e).
The best moving-average window length for time-averaged odor concentrations (7 timesteps or 0.3s on average across all 5 agents) is significantly shorter than that for time-averaged odor encounters (47 timesteps or 1.9s on average across all 5 agents). 
Time-averaged odor concentrations are also better encoded ($R^2$=0.91 on average across 5 agents) than time-averaged odor encounters ($R^2$=0.59 on average across 5 agents).
See Extended Data Table~\ref{table_supp_ewma} for data on each of the 5 RNN agents.


\subsection*{RNN Connectivity Analysis}
The update rule for a Vanilla RNN with hidden state vector $\bh_t$ is given by
\begin{align*}
    \bh_t = F (\bh_{t-1}, \bx_t) = \tanh \left(\bW_h \bh_{t-1} + \bW_x \bx_t + b \right),
\end{align*}
where $\bW_h$ is recurrence (connectivity) matrix of the hidden layer,
$\bx_t$ are the network's inputs,
$\bW_x$ is the input-to-hidden layer matrix,
and $b$ is a bias term \citep{sussillo2013opening}.
Next, we consider a linearization of this nonlinear system around arbitrary expansion points. 
The RNN update equation can be linearized around an arbitrary expansion point $\left(\mathbf{h}^{\mathrm{e}}, \mathbf{x}^{\mathrm{e}}\right)$ to get a linear dynamical system approximated by: 
\begin{align*}
\mathbf{h}_{t} \approx & F\left(\mathbf{h}^{\mathrm{e}}, \mathbf{x}^{\mathrm{e}}\right) \\
&+ \left.\mathbf{J}^{\mathrm{rec}}\right|_{\left(\mathbf{h}^{\mathrm{e}}, \mathbf{x}^{\mathrm{e}}\right)} \Delta \mathbf{h}_{t-1}+\left.\mathbf{J}^{\mathrm{inp}}\right|_{\left(\mathbf{h}^{\mathrm{e}}, \mathbf{x}^{\mathrm{e}}\right)} \Delta \mathbf{x}_{t},
\end{align*}
where $\Delta \mathbf{h}_{t-1}=\mathbf{h}_{t-1}-\mathbf{h}^{\mathrm{e}}$ is the state of the linearized system, 
\mbox{$\Delta \mathbf{x}_{t}=\mathbf{x}_{t}-\mathbf{x}^{\mathrm{e}}$} is the linearized system's input,
$\mathbf{J}^{\mathrm{rec}}$ is the recurrence Jacobian, and $\mathbf{J}^{\mathrm{rec}}$ is the input Jacobian \citep{maheswaranathan2019reverse}.
To be explicit,
\begin{align*}
\left.J_{i j}^{\mathrm{rec}}\right|_{\left(\mathbf{h}^{\mathrm{}{e}}, \mathbf{x}^{\mathrm{e}}\right)} =& \frac{\partial F(\mathbf{h}, \mathbf{x})_{i}}{\partial h_{j}}, \\ 
\left.J_{i j}^{\mathrm{inp}}\right|_{\left(\mathbf{h}^{e}, \mathbf{x}^{e}\right)}=&\frac{\partial F(\mathbf{h}, \mathbf{x})_{i}}{\partial x_{j}}.
\end{align*}
Note that 
\mbox{$\left.\mathbf{J}^{\mathrm{rec}}\right|_{\left(0, 0\right)} = \bW_h$} and
\mbox{$\left.\mathbf{J}^{\mathrm{inp}}\right|_{\left(0, 0\right)} = \bW_x$}.

Prior literature has looked at the eigenvalues and eigenvectors of the recurrence Jacobian (and recurrence matrix) to investigate how connectivity affects the dynamics of the network \citep{rajan2006eigenvalue,maheswaranathan2019reverse}.  
Specifically \citep{maheswaranathan2019reverse} obtains the stimulus integration timescale $\tau_i$ associated with a stable eigenvalue $\lambda_i$ (i.e. $|\lambda_i| \leq 1$), by looking at the discrete-time iteration \mbox{$h_i(t)=\lambda_i^{t} h_i(0)$} that governs the integration of stimulus in the direction of eigenvector $v_i$ associated with $\lambda_i$.
They then compare this with the equivalent continuous time equation 
\mbox{$h_i(t)=h_i(0) e^{-t / \tau_i}$}, to get 
\mbox{$\tau_i = \left| ( 1/ \ln|\lambda_i| ) \right|$}.
Following their approach, we consider the eigenvalues of the recurrence Jacobian and associated stimulus integration timescales along the trajectories of several episodes.
This timescale governs the integration of stimuli in the direction of the corresponding eigenvectors.
We chose at random one successful and one unsuccessful episode from each of three plume configurations (`constant,' `switch-once,' and `switch-many').
At each time step of the trajectory, we computed the recurrence Jacobian assuming zero input $\left.\mathbf{J}^{\mathrm{rec}}\right|_{(\bh,0)}$.
% The stimulus integration timescale $\tau_i$ associated with a stable eigenvalue $\lambda_i$ (i.e. $|\lambda_i| \leq 1$) can be interpreted as a timescale with the conversion \mbox{$\tau_i = \left| ( 1/ \ln|\lambda_i| ) \right|$}.



\subsection*{Related work}
In the field of neural computation, an emerging body of work has used DRL to train ANNs that solve tasks closely inspired by tasks from neuroscience.
For instance, agents have been trained to study learning and dynamics in the motor-cortex \citep{weinstein2017structure, song2020deep}, time encoding in the hippocampus \citep{lin2021time}, reward-based learning and meta-learning in the pre-frontal cortex \citep{song2017reward, wang2018prefrontal, botvinick2019reinforcement}, and task-associated representations across multiple brain areas \citep{cross2021using}.
There have been several recent perspectives articulating the relevance of this emerging algorithmic toolkit to neuroscience \citep{botvinick2020deep,gershman2020neurobiology} and ethology \citep{crosby2020building}.

Our work is most directly related to three recent research efforts.
\cite{merel2019deep} developed a virtual-reality model of a rodent embodied in a skeleton body and endowed with a deep ANN `brain.' 
They trained this model using DRL to solve four tasks and then analyzed the virtual rodent's emergent behavior and neural activity, finding similarities at an abstract level between their agent and observations from rodent studies. 
\cite{reddy2021sector} studied the trail tracking strategies of terrestrial animals with one (e.g. one antenna) or two (e.g. two nostrils) odor sensors. 
They found that RL agents trained on simulated trails recapitulate the stereotypical zig-zagging tracking behavior seen in such animals. 
Using a static trail model and an explicit (not neural) probabilistic model for sensory integration, they studied the effect of varying agent and task parameters on the emergent stereotypical zig-zagging behavior.     
\cite{rapp2020spiking} used a biologically detailed spiking neural circuit model of a fly mushroom body to study sensory processing, learning, and motor control in flying insects when foraging within turbulent odor plumes. 

We build on the approach of these recent papers that study artificial agents solving neural inspired tasks, and our work is also distinct in several key ways. 
First, we simulate a more computationally challenging task than the static trail tracking task in \citep{reddy2021sector}, because our odor environment is configurable, dynamic, and stochastic.
In contrast, \cite{rapp2020spiking} use a similar plume environment with only constant wind-direction plumes, but with the added complexity of a secondary distractor odor that their agent must learn to avoid.
Second, we have made several simplifications and abstractions that make analysis more tractable, so that we may focus on the general principles behind plume tracking.
Specifically, we omit biomechanical details, impose no biologically inspired connectivity constraints, and do not use spiking neurons.
Instead, our networks are `Vanilla' RNNs (rather than the gated RNNs used in \cite{merel2019deep} or the spiking neurons in \cite{rapp2020spiking}), which facilitates analyses from the dynamical systems perspective \citep{rajan2006eigenvalue,sussillo2013opening,maheswaranathan2019reverse,maheswaranathan2019universality,vyas2020computation}.
We analyze emergent behaviors and neural dynamics at the network level, which provides us with an abstract understanding of task-relevant neural computations that is robust to small changes in network architecture and training hyperparameters \citep{vyas2020computation, maheswaranathan2019universality, sussillo2013opening}.
Lastly but importantly, since we do not model vision or joint-level motor control as in \cite{merel2019deep}, our neural networks are simpler and can be trained on a computational budget accessible to an academic lab. 



%%%%%%%%%%%%%%s
\clearpage
\section*{Data availability statement}
The datasets generated during and analysed during the current study are publicly available in the Figshare repository: \\ \url{https://doi.org/10.6084/m9.figshare.16879539.v1}.

\section*{Code availability statement}
Animations, code, data and instructions to reproduce all figures and results in this manuscript can be found at: \\ \url{https://github.com/BruntonUWBio/plumetracknets}.
All code has been open-sourced under the MIT Licence.


%%%%%%%%%%%%%%s
\section*{Acknowledgements}
We thank Aravind Rajeswaran, Niru Maheswaranathan, Stefano Recanatesi, Scott Sterrett, and Steve Brunton for helpful comments and discussions.
The plume tracking task graphic in Figure \ref{fig_training} is inspired by a similar figure in \cite{baker2018algorithms}, and uses parts of an open source fruitfly graphic from scidraw.io \citep{costa_gil_2020_3926137}.
We are grateful for the well documented open source implementation of PPO by Ilya Kostrikov \citep{kostrikov2018pytorch}, which we heavily adapted and built upon for our work.

This work has been funded by 
the Air Force Research Lab award FA8651-20-1-0002 and FA9550-21-0122 (FvB); 
the National Institutes of Health award NIH P20GM103650 (FvB);
the Defense Advanced Research Projects Agency HR001120C0021 (RPNR); 
the Templeton World Charity Foundation (RPNR); 
the National Science Foundation award EEC-1028725 (RPNR); 
the Air Force Office of Scientific Research awards FA9550-19-1-0386 and FA9550-18-1-0114 (BWB); 
and the Washington Research Foundation (BWB).


%%%%%%%%%%%%%%s
\section*{Author Contributions}
SHS, FvB, RPNR and BWB conceived of the study/analysis. 
SHS engineered the agents. 
SHS performed the data analysis. 
SHS, FvB, RPNR, and BWB interpreted the results. 
SHS and BWB wrote the manuscript. 
All authors reviewed and edited the manuscript. 

%%%%%%%%%%%%%%s
\section*{Declaration of interests}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.



%%%%%%%%%%%%%%
% \newpage
\clearpage
\small
% \bibliographystyle{abbrv}
% \bibliographystyle{alpha}
% \bibliographystyle{ieeetr} % Order items by appearance
% \bibliographystyle{apalike}
\bibliographystyle{elsarticle-harv}
\bibliography{plume}
% \printbibliography


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\section{EXTENDED DATA} 
[Up to 10 figure or table panels, each of which must be referenced in the main text]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \clearpage
% \subsection{Key parameters for simulation, agent, model, and training and evaluation}
% \label{sec_supp_hyperparams}

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Parameter description} & \textbf{Value/Range} \\
     \hline   
       Simulation integration time-step & 0.01s  \\ \hline
       Wind speed & 0.5 m/s  \\  \hline
       Wind speed crosswind noise & $\mathcal{N}(0, 0.005)$ m/s (per timestep)  \\ \hline
       Puff birth rate (Poisson mean) & 1.0 puffs/timestep (at 100 FPS)  \\ \hline
       Puff initial radius & 0.01m  \\ \hline
       Puff radius growth rate (radial diffusion-rate) & 0.01m/s (= 1.0x)  \\ \hline
       Maximum plume extent simulated (x, y) & (-2/+10m, $\pm$ 5m)  \\ \hline
     \hline
    \end{tabular}
    \caption{Plume parameters}
    \label{table_supp_plume}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Parameter description} & \textbf{Value/Range} \\
     \hline   
       Environment frame rate  & 25 FPS  \\ \hline
       Sensor sampling rate & 25 Hz  \\ \hline
       Forward movement capacity ($\Delta_{max}$) & 2.5 m/s  \\ \hline
       Turn capacity ($\theta_{max}$) & $\pm$ 6.25 $\pi$ radians/s ($\pm$ 1125 $^\circ$/sec)  \\ \hline
       Homing radius & 0.2 m  \\ \hline
       Max. stray from plume allowed & 2 m  \\ \hline
       Odor sensing thresholds (minimum, maximum) & (0.0001, 1.0) (A.U).  \\ \hline
     \hline
    \end{tabular}
    \caption{Agent and environment parameters}
    \label{table_supp_agent_env}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Parameter description} & \textbf{Value/Range} \\
     \hline   
       RNN hidden layer width & 64 units  \\ \hline
       Feedforward hidden layer width(s) & 64 units  \\ \hline
       Neural network nonlinearity & tanh  \\ \hline
       Layer initialization (Recurrent, Feedforward) & (Normal, Orthogonal)  \\ \hline
     \hline
    \end{tabular}
    \caption{Model (neural network) parameters}
    \label{table_supp_nn}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Parameter description} & \textbf{Value/Range} \\
     \hline   
        RNN training steps & 5M  \\ \hline
        MLP training steps & 2M  \\ \hline
        Learning Rate & 0.0003 (with linear decay) \\ \hline
        % Weight Decay (L2 regularization) & 0.0 \\ \hline
        PPO Entropy Coefficient & 0.05 \\ \hline
        PPO Value Loss Coefficient & 0.5 \\ \hline
        PPO Epochs & 10 \\ \hline
        PPO Gamma & 0.99 \\ \hline
        PPO max. gradient norm & 0.5 \\ \hline
        GAE Lambda & 0.95 \\ \hline
        GAE steps & 2048 \\ \hline
     \hline
    \end{tabular}
    \caption{Training algorithm, training curriculum and evaluation parameters}
    \label{table_supp_training}
\end{table}

\clearpage
% \subsection{Behavior module metadata distributions}
% \label{sec_supp_module}

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Agent} & \textbf{Agent ID} & \textbf{Lost module threshold}   \\
    %  & & \textbf{[timesteps]}   \\
     \hline   
        RNN 1 & 2760377 & 30 steps (1.2 s) \\ \hline
        RNN 2 & 3199993 & 25 steps (1.0 s) \\ \hline  
        RNN 3 & 3307e9 & 35 steps (1.4 s) \\ \hline
        RNN 4 & 541058 & 38 steps (1.52 s) \\ \hline
        RNN 5 & 9781ba & 25 steps (1.0 s) \\ \hline
     \hline
    \end{tabular}
    \caption[Thresholds defining behavioral module changes]{Thresholds for defining when the \textit{lost} behavior module kicks in i.e. duration (in timesteps or seconds) since the plume was last encountered.}
\label{table_supp_module_threshold}
\end{table}

% \subsection{Neural activity dimensionality and neural representations}
\begin{table}[h!]
    \centering
    \begin{tabular}{cccccc}
     \hline\hline
     \textbf{Agent} & 
     \textbf{Agent ID} & 
     $odor_{EWMA}$ & 
     $odor_{EWMA}$ & 
     $odor_{ENC}$ & 
     $odor_{ENC}$  \\
     & 
     &
     window [steps] & 
     $R^2$ &
     window [steps] &
     $R^2$ \\
     \hline   
        RNN 1 & 2760377 & 8  & 0.91 & 62 & 0.57 \\ \hline
        RNN 2 & 3199993 & 10 & 0.86 & 44 & 0.71 \\ \hline  
        RNN 3 & 3307e9  & 8  & 0.92 & 46 & 0.57 \\ \hline
        RNN 4 & 541058  & 6  & 0.88 & 40 & 0.51 \\ \hline
        RNN 5 & 9781ba  & 12 & 0.91 & 44 & 0.59 \\ \hline
     \hline
    \end{tabular}
    \caption{Moving window lengths and linear regression fit $R^2$ for two represented variables: $odor_{EWMA}$ and $odor_{ENC}$. 
    % See Section \ref{sec_repr} for further details. 
    (Recall that 25 timesteps = 1.0 second).
    }
\label{table_supp_ewma}
\end{table}
% (0.91 + 0.86 + 0.92 + 0.88 + 0.91)/5 = 0.90
% (0.57+0.71+0.57+0.51+0.59)/5 = 0.59

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Agent} & \textbf{Agent ID} & \textbf{Test set accuracy} & \textbf{Test set accuracy} & \textbf{Test set accuracy}   \\
     & & \textbf{(All features)} & \textbf{(Instantaneous only)} & \textbf{(Most freq. class)}   \\
     \hline   
        RNN 1 & 2760377 & 0.84 & 0.74 (0.10) & 0.33 (0.51) \\ \hline
        RNN 2 & 3199993 & 0.67 & 0.49 (0.18) & 0.28 (0.39) \\ \hline  
        RNN 3 & 3307e9 & 0.82 & 0.69 (0.13) & 0.39 (0.43) \\ \hline
        RNN 4 & 541058 & 0.70 & 0.53 (0.17) & 0.44 (0.26) \\ \hline
        RNN 5 & 9781ba & 0.84 & 0.74 (0.10) & 0.40 (0.44) \\ \hline
     \hline
    \end{tabular}
    \caption{Classifier based quantification of contribution of represented features: 
    In last two columns, quantity in parentheses is the difference in accuracy with respect to classifier that has all features (4 represented features and instantaneous egocentric sensory features). 
    Represented features contribute to higher test accuracy. 
    % See Section \ref{sec_repr} for further details.
    }
\label{table_supp_repr}
\end{table}


% 2760377: top5taus -- [116.5  81.5  16.9  13.5   8.3]
% 3199993: top5taus -- [95.7 61.7 16.6 12.   9.6]
% 3307e9: top5taus -- [56.5 13.   7.7  6.8  5.8]
% 541058: top5taus -- [86.4 51.8 15.1 12.4  9.7]
% 9781ba: top5taus -- [86.2 27.4  8.6  6.6  5.6]

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Agent} & \textbf{Agent ID} & \textbf{Top 5 $\tau$s}  \\
     \hline   
        RNN 1 & 2760377 & 116.5, 81.5, 16.9, 13.5, 8.3  \\ \hline
        RNN 2 & 3199993 & 95.7, 61.7, 16.6, 12.0, 9.6  \\ \hline  
        RNN 3 & 3307e9 & 56.5, 13.0, 7.7, 6.8, 5.8  \\ \hline
        RNN 4 & 541058 & 86.4, 51.8, 15.1, 12.4, 9.7  \\ \hline
        RNN 5 & 9781ba & 86.2, 27.4, 8.6, 6.6, 5.6  \\ \hline
     \hline
    \end{tabular}
    \caption{Top 5 $\tau$s (stimulus integration timescales) for each RNN seed}
\label{table_supp_taus}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\section{SUPPLEMENTARY INFORMATION} 

[
Online supplementary information; 
No word/panel limits; 
Will be linked to article and online supplementary code/data. 
We repeat some details from the main text for the sake of readability.
]

\subsection{Supplementary details on agent training and evaluation}
\label{sec_supp_train_eval}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{fig_training_plume.pdf}
\caption{\textbf{Snapshot of training plume:} 
Plume (dull grey) originating at crosshairs (dotted black lines, bottom left).
Current wind direction shown by arrow in dashed circle (bottom left).
50 randomly chosen initialization points (red) overlayed on plume.
Agent is initialized with a uniformly randomly chosen head-direction at a random location near or on the plume. 
Wind direction switches by a random amount at random times as described in Section \ref{supp_plumegen}
}
\label{training_plume}
\end{figure*}



\noindent \subsubsection{Plume pre-computation:}
\label{supp_plumegen}
Puffs are generated at the source located at $(0,0)$, at the rate of $r_t \sim Poisson(R)$ puffs/step, where $R=1.0$.
Each puff's location $p_t = (x_t,y_t)$ is henceforth governed by the stochastic differential equation, $p_t = p_{t-1} + w_t \delta + \xi$, where $w_t$ is the wind-velocity at time $t$,   $\xi \sim \mathcal{N}(0, \sigma)$ is cross wind i.i.d. random Gaussian noise added per the turbulent plume model of \citep{farrell2002filament}. 
Each puff trajectory is integrated using a simple forward Euler integrator at 100 frames/sec.
Furthermore, each puff starts with a radius $r_0 = 0.01$ m and undergoes a diffusion process that increases it's radius at the rate of $0.01$ m/s.
Puff concentrations reduce in proportion to their increase in volume due to the increased radius (see Methods for formula). 

We compute 120-second long (clock time) plumes ahead of training/evaluation time for the `constant',  `switch-once' and `switch-many' configurations.
A 40s window (60s - 100s) of the `switch-many' and `constant' plumes are used for training agents (Figure \ref{training_plume}).
`sparse' plumes are simulated by downsampling the number of puffs simulated in a `constant' plume simulation.
`sparser' plumes are `sparse' plumes with reduced (0.5x) radial diffusion-rates to encourage more \textit{patchy} plume landscapes.

Wind velocity $w_t$ is held constant at $(0.5, 0.0)$ m/s for all $t$ for the `constant' plume configuration.
For the `switch-once' plume configuration, wind velocity $w_t$ starts at $(0.5, 0.0)$ m/s till $t=60.00$ s, when it makes a single $45^{\circ}$ counter-clockwise turn and stays there for the rest of the simulation.
For the `switch-many' plume configuration, wind velocity $w_t$ changes once every $3.0 + \tau$ seconds, where $\tau \sim Uniform(-0.3, +0.3)$ is a random shift added i.i.d. at each change.
wind direction turns are sampled i.i.d. from a $\mathcal{N}(0^{\circ}, 30^{\circ})$ Gaussian distribution truncated at $\pm60^{\circ}$.

\noindent \subsubsection{Training}

\textbf{Partially Observable Markov Decision Process (POMDP):} 
To train agents using DRL, we define a Partially Observable Markov Decision Process (POMDP) \citep{sutton2018reinforcement} as follows (also see Figure~\ref{fig_training}). 
While a belief-state is not explicitly defined, we use an RNN (or MLP with past inputs) to learn and estimate a belief-state-like representation from the history of past observations. 
Although an explicit representation of uncertainty is not computed in our model,  the RNN state representation, when combined with reinforcement learning, allows the model to approximate a solution to the underlying POMDP problem.

\begin{itemize}
    \item 
    \textit{Action space:} Agents provide a two dimensional output $a_t$ at each timestep corresponding to how much they want to turn and how much they want to move forward.
    $$ \mathbf{a_t} = [a_{\theta}, a_m], \text{where } a_{\theta} \in [-\theta_{max}, +\theta_{max}], a_m \in [0, \Delta_{max}]$$ 
    The maximum turn capacity of an agent ($\theta_{max}$) is $6.25\pi$ radians/s (1125 $^\circ$/s), and the maximum forward movement capacity ($\Delta_{max}$) of an agent is 2.5 m/s.
    
    \item 
    \textit{Observation space:} Agents receive a 3-dimensional egocentric sensory observation vector $o_t$ at timestep $t$, comprising odor-concentration and (x, y) components of relative wind-velocity at the agent's current location and orientation in the plume.
    Note that the agent's current location and orientation in the plume are tracked and updated by the training environment code. 
    $$ \mathbf{o_t} = [o_c, o_x, o_y], \text{where } o_c \in [c_{min}, c_{max}], o_x,o_y \in [-(\Delta_{max} + |v_{wind}|), (\Delta_{max} + |v_{wind}|)]$$ 
    Here $c_{min}$ and $c_{max}$ are the minimum and maximum perceivable odor concentrations, that have been manually set to be 0.0001 and 1.0 arbitrary units respectively. 

    \item 
    \textit{Reward function:}:
    Rewards are given to encourage task completion, i.e. home in on the plume source.
    The agent receives:
    $+100$ when it reaches within a small fixed radius $r_{homed} = 0.2m$ of the source, 
    $-\epsilon$ per timestep to simulate a `metabolic cost' to flying and therefore encourage faster homing.
    We also provide the agent two shaping rewards, without which the training process is infeasibly slow:
    First, a reward proportional to the decrease in radial distance to the source per timestep $(r_{t-1} - r_{t})$ as a form of shaping reward.
    Here, $r_{t} = \sqrt{x_t^2 + y_t^2}$ is the euclidean distance of the agent to the source at timestep $t$.
    Second, a fixed negative reward of $-10$ if the agent strays more than $r_{stray} = 2$m away from the plume (i.e. the center of the nearest puff is greater than $r_{stray}$). 
    
    \item 
    \textit{Transition function:} 
    The agent's location and orientation within the arena is randomly initialized at the beginning of each training episode (see Figure \ref{training_plume} for example locations).
    The environment then deterministically updates the agent's location and orientation at each timestep taking into account its actions and the wind velocity. 
    Episodes end if the agent reaches within a radial distance $r_{homed}$ of odor source, or if the agent strays more than $r_{stray}$ from the plume, or if the episode exceeds 300 timesteps (12 seconds of clock time).
    
    \item 
    \textit{Augmented observation space for MLPs:}
    To understand the role of memory on tracking performance (Ref. Figure \ref{fig_eigen_mlps}), we use feedforward-only networks (MLPs) with fixed-length memory. 
    Memory is simulated by appending historical sensory observations into the MLPs' inputs (known as `frame stacking' in the DRL literature \citep{mnih2013playing}).
    Therefore $\mathbf{o_t}$ for an MLP with $L$ timesteps history is now $[o^{(0)}_{c}, o^{(0)}_{x}, o^{(0)}_{y}, \dots o^{(L)}_{c}, o^{(L)}_{x}, o^{(L)}_{y} ] $.

\end{itemize}

Observations are received from the environment, processed and acted upon by the agents at each timestep (25 FPS or every 40ms).
We implement the POMDP environment using the OpenAI Gym \citep{brockman2016openai} and stable-baselines \citep{hill2018stable} libraries. 
\\

\textbf{Training curricula:}
We adapt an open source implementation \citep{kostrikov2018pytorch} of the Proximal Policy Gradient algorithm with Generalized Advantage Estimation (PPO-GAE) \citep{schulman2015high,schulman2017proximal} to train our agents.

To train our agents to perform across dynamically varying plumes, we randomize the agent's location, agent's  orientation, plume state and plume sparsity at the start of each training episode.
% wind direction and odor puff sparsity conditions  
Agents are initialized at random starting locations $(x,y)$, where x is chosen uniformly randomly in the range $[30, 80]$ percentile of puff locations;
y is chosen by sampling from a normal distribution with mean given by the median y-coordinate of odor puffs in the range $[x-1, x+1]$, and variance given by the $5^{th} - 50^{th}$ percentile y-coordinate difference of the aforementioned odor puffs.
Initial agent orientation is selected at random from [$-\pi, \pi$] radians.
The `switch-many' plume, which changes direction every $\approx 3$ seconds, is used for training.
Initial plume state is randomized by choosing a random time between 60s - 90s, at which to initialize the precomputed plume.
The simulation is sparsified by downsampling the number of puffs to a fraction randomly uniformly chosen in the range $[0.3, 1.0]$. 
The plume is randomly flipped about the x-axis to mitigate any y-directional biases that might have crept into the finite plume simulation. 

Curriculum based training methods are known to improve training performance by gradually increasing the difficulty of the training task over the course of the training process \citep{bengio2009curriculum}.
We train our RNNs using a two stage curriculum, where we first train the RNN for 1 million timesteps on the constant wind direction plume, and then train it for another 4 million timesteps on the 'switch-many' plume.
This two stage process improved the stability and performance of the training process for RNNs, but not for MLPs.
MLPs are directly trained for 2 million timesteps on the 'switch-many' plume. 
Training durations have been chosen such that training updates reliably converge within these times. \\

\textbf{Hyperparameter selection}:
Our training process has hyperparameters relating to (1) training algorithm hyperparameters, (2) training plume parameters, and (3) neural network architecture. 
(See Extended Data Tables \ref{table_supp_plume}--\ref{table_supp_training} for a list of all [hyper]parameters and values).
While PPO is not the most sample efficient algorithm, it is known to work robustly across a wide range of continuous control (continuous observation and action space) problems without needing extensive hyperparameter tuning \citep{schulman2017proximal,ni2021recurrent}.
Furthermore, exhaustive hyperparameter tuning is computationally unfeasible on our budget.
However, we do try to tweak hyperparameters one-by-one starting off from the parameters suggested in the PPO manuscript for continuous control problems.

To decide network width, we trained RNNs with 24, 32, 64, 96, 128 and 256 units, and found no improvement in performance by increasing width beyond 64 units. We found that training for networks with 32 or 24 units converged less often and their test performance was lower than wider networks. This is expected because there's often a "minimum capacity" of network required to easily train a network for a task; see, for example, the simple benchmark tasks (with 3-dimensional inputs) and networks used in \cite{maheswaranathan2019universality}.

For RNN-MLP comparisons, since we are trying to artificially limit the amount of memory available to the agent, we chose to vary the ``input half'' of the network, and keep the architecture of the actor and critic sub-networks unchanged. To do this, we had to keep the RNN and MLP layer widths identical. We believe that this makes the comparison between RNNs and MLPs fairer since the state representation dimension (i.e. the RNN hidden state dimension, or the MLP intermediate layer width) remains the same across networks.
Finally, we also trained Gated Recurrent Units (GRUs) in the same manner as we did our Vanilla RNNs (RNNs), and found that the performance of the GRUs did not significantly exceed that of the RNNs (see Figure \ref{fig_supp_gru}). 

For MLPs, we limit ourselves to 12 timesteps of memory (historical observations) because we observe that MLP performance improvement plateaus around 10 to 14 timesteps of memory, and then reduces as we increase memory. It is known that the optimization problem grows exponentially harder with increase in the size of the input space. RNNs do not require any history stacking (and store memory in the form of internal state) and are therefore preferred over MLPs for precisely this reason when the problem involves long-timescales.
\\

\textbf{Other shaping rewards explored}: Flying insects are known to exhibit a significant range of speeds \citep{van2014plume}. 
However, our trained agents mostly fly at either their maximum speed or very slowly (see Figure \ref{fig_regimes_coursedir}).
As additional reward shaping, we did try to add movement-related penalties to the reward function to induce some speed modulation, however, did not use these agents because of drastically worse performance compared to unpenalized agents. 
Future work could explore ways of skewing DRL reward functions towards such auxiliary goals that are not aligned with the primary plume tracking task. \\

\textbf{Computational resources}: All models are trained and evaluated on an Ubuntu Linux v20.04 workstation with Intel Core i9-9940X CPU and a TITAN RTX GPU.
Each seed takes $\approx 16$ hours to train and evaluate, with MLP and RNN models using 1 and 4 cores in parallel respectively. \\



\subsubsection{Evaluation}
\label{sec_supp_eval}
We evaluated trained agents over a behavioral assay comprising fixed set of initial locations, initial simulation timestamps and initial agent directions across the aforementioned plume wind direction and birth-rate configurations, each comprising 240 episodes.

The same set of $240$ initial conditions for each episode are used to initialize the agent and simulator, for each agent and dataset evaluated: 
\begin{itemize}
    \item Initial agent head angle (with respect to ground): $0, \frac{1}{4}\pi, \frac{1}{2}\pi, \frac{3}{4}\pi, \pi, \frac{5}{4}\pi, \frac{3}{2}\pi, \frac{7}{4}\pi$ radians
    \item Initial x-coordinate: 4, 6, and 8 meters
    \item Initial y-coordinate: 0$^{th}$, 25$^{th}$, 50$^{th}$, 75$^{th}$, and 100$^{th}$ percentile of the minimum and maximum y-coordinate of the puffs located in a 1-meter band around the initial x-coordinate. 
    For 'constant'  wind direction plumes (including sparse plumes), the task is made harder by selecting only 0$^{th}$, 50$^{th}$, and 100$^{th}$ percentiles as described before (i.e. $y_{min}, y_{median}, y_{max}$) and then adding two other locations that are $\pm 0.5$ m outside the plume (i.e. $y_{min} - 0.5$ m and $y_{max} + 0.5$ m)]
    \item Initial timestamp: $60.00$s and $61.00$s ($58.00$s and $59.00$s for the 'switch-once' plume as it switches at exactly $60.00$s)
\end{itemize}

\textbf{Agent selection}:  We train 14 seeds per model type (RNNs, and MLPs with 2, 4, 6 ..., 12 timesteps of history) and select the top-5 best performing seeds for analysis.
Performance here is measured by counting the number of successful episodes across `constant', 'switch-once' and 'switch-many' plumes. \\ 

\textbf{Evaluation subset}: For all our analyses in Results, we use a randomly selected 120 episode \textit{evaluation subset} of the 240 evaluation episodes for each of the constant, switch-once and switch-many plume configurations.
The selected episodes are balanced to include an equal number (60 episodes each) of successful and unsuccessful plume tracking episode outcomes.
Whenever there are fewer than 60 episodes of either outcome type (successful or unsuccessful) for any plume configuration, then the selection is trimmed to use an equal number of episodes of the smaller outcome type.
PCAs tend to be sensitive to imbalances in the data, and this balancing process enables visualizations to be consistently compared across agents.  
In the analysis described in Figure \ref{fig_eigen_mlps}c-e, we use all 240 evaluation episodes per agent. \\



\begin{figure}[h!]
\centering
\includegraphics[width=0.45\linewidth]{home_by_arch_VRNN_GRU.png}
\caption{Comparison of Vanilla RNNs and GRUs across 4 plume configurations. Vanilla RNN data is same as that in Figure \ref{fig_eigen_mlps}}.
\label{fig_supp_gru}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\subsection{Behavior module metadata distributions}
\label{sec_supp_module}

% \begin{table}[h!]
%     \centering
%     \begin{tabular}{ccccc}
%      \hline\hline
%      \textbf{Agent} & \textbf{Agent ID} & \textbf{Lost module threshold}   \\
%     %  & & \textbf{[timesteps]}   \\
%      \hline   
%         RNN 1 & 2760377 & 30 steps (1.2 s) \\ \hline
%         RNN 2 & 3199993 & 25 steps (1.0 s) \\ \hline  
%         RNN 3 & 3307e9 & 35 steps (1.4 s) \\ \hline
%         RNN 4 & 541058 & 38 steps (1.52 s) \\ \hline
%         RNN 5 & 9781ba & 25 steps (1.0 s) \\ \hline
%      \hline
%     \end{tabular}
%     \caption[Thresholds defining behavioral module changes]{Thresholds for defining when the \textit{lost} behavior module kicks in i.e. duration (in timesteps or seconds) since the plume was last encountered.}
% \label{table_supp_module_threshold}
% \end{table}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{regime_histos_2760377.png}
\caption[Behavior modules - Agent 1]{Behavior modules - Agent 1 (See Figure \ref{fig_regimes_coursedir} for equivalent data on Agent 3 and figure details)}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{regime_histos_3199993.png}
\caption[Behavior modules - Agent 2]{Behavior modules - Agent 2}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{regime_histos_3307e9.png}
\caption[Behavior modules - Agent 3]{Behavior modules - Agent 3 (same as Figure \ref{fig_regimes_coursedir})}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{regime_histos_541058.png}
\caption[Behavior modules - Agent 4]{Behavior modules - Agent 4}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.85\linewidth]{regime_histos_9781ba.png}
\caption[Behavior modules - Agent 5]{Behavior modules - Agent 5}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\subsection{Agents track plume centerline, not current wind direction}
\label{sec_supp_centerline}



\begin{figure*}[h!]
\centering
\includegraphics[width=0.28\linewidth]{regime_dists_2760377_constantx5b5_TRACK_CD.png}
\includegraphics[width=0.28\linewidth]{regime_dists_2760377_switch45x5b5_TRACK_CD.png}
\includegraphics[width=0.405\linewidth]{regime_dists_2760377_noisy3x5b5_TRACK_CD.png}
\caption{Empirical course-direction (CD) distribution - Agent 1 (See Figure \ref{fig_regimes_coursedir} for equivalent data on Agent 3 and figure details)} 
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.28\linewidth]{regime_dists_3199993_constantx5b5_TRACK_CD.png}
\includegraphics[width=0.28\linewidth]{regime_dists_3199993_switch45x5b5_TRACK_CD.png}
\includegraphics[width=0.405\linewidth]{regime_dists_3199993_noisy3x5b5_TRACK_CD.png}
\caption{Empirical course-direction (CD) distribution - Agent 2} 
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.28\linewidth]{regime_dists_3307e9_constantx5b5_TRACK_CD.png}
\includegraphics[width=0.28\linewidth]{regime_dists_3307e9_switch45x5b5_TRACK_CD.png}
\includegraphics[width=0.405\linewidth]{regime_dists_3307e9_noisy3x5b5_TRACK_CD.png}
\caption[Agent 3: Empirical course-direction (CD) distribution]{ 
Empirical course-direction (CD) distribution - Agent 3 (Same as Figure \ref{fig_regimes_coursedir}) 
}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.28\linewidth]{regime_dists_541058_constantx5b5_TRACK_CD.png}
\includegraphics[width=0.28\linewidth]{regime_dists_541058_switch45x5b5_TRACK_CD.png}
\includegraphics[width=0.405\linewidth]{regime_dists_541058_noisy3x5b5_TRACK_CD.png}
\caption{Empirical course-direction (CD) distribution - Agent 4} 
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.28\linewidth]{regime_dists_9781ba_constantx5b5_TRACK_CD.png}
\includegraphics[width=0.28\linewidth]{regime_dists_9781ba_switch45x5b5_TRACK_CD.png}
\includegraphics[width=0.405\linewidth]{regime_dists_9781ba_noisy3x5b5_TRACK_CD.png}
\caption{Empirical course-direction (CD) distribution - Agent 5} 
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\subsection{Neural activity dimensionality and neural representations}
\label{sec_supp_repr}


% \textbf{Odor encounters}: Our definition of odor encounters is identical to that used in \cite{demir2020walking}. 
% The stream of odor inputs is discretized to be 1 at the first timestep of the stream where the odor is perceptible and 0 for the remaining contiguous steps where it is still perceptible. 
% Other processing is as described in Section \ref{sec_repr}.


% \begin{table}[h!]
%     \centering
%     \begin{tabular}{cccccc}
%      \hline\hline
%      \textbf{Agent} & 
%      \textbf{Agent ID} & 
%      $odor_{EWMA}$ & 
%      $odor_{EWMA}$ & 
%      $odor_{ENC}$ & 
%      $odor_{ENC}$  \\
%      & 
%      &
%      window [steps] & 
%      $R^2$ &
%      window [steps] &
%      $R^2$ \\
%      \hline   
%         RNN 1 & 2760377 & 8  & 0.91 & 62 & 0.57 \\ \hline
%         RNN 2 & 3199993 & 10 & 0.86 & 44 & 0.71 \\ \hline  
%         RNN 3 & 3307e9  & 8  & 0.92 & 46 & 0.57 \\ \hline
%         RNN 4 & 541058  & 6  & 0.88 & 40 & 0.51 \\ \hline
%         RNN 5 & 9781ba  & 12 & 0.91 & 44 & 0.59 \\ \hline
%      \hline
%     \end{tabular}
%     \caption{Moving window lengths and linear regression fit $R^2$ for two represented variables: $odor_{EWMA}$ and $odor_{ENC}$. 
%     See Section \ref{sec_repr} for further details. 
%     (Recall that 25 timesteps = 1.0 second).
%     }
% \label{table_supp_ewma}
% \end{table}
% % (0.91 + 0.86 + 0.92 + 0.88 + 0.91)/5 = 0.90
% % (0.57+0.71+0.57+0.51+0.59)/5 = 0.59

% \textbf{Agent action classifier}: To quantify how important these represented variables are to actual task performance, we train a Random Forest (RF) \citep{breiman2001random} classifier to predict actions taken by the agent over successful trajectories.
% We uniformly partition the Turn and Move action variable, which are continuous valued, into domains of 3 and 2 discrete classes respectively.
% These classes correspond roughly to `left', `center' and `right' turns, and to `fast' and `slow' forward movements.
% These are concatenated to form a 6-class independent variable.
% The classifier receives instantaneous sensory observations (egocentric wind speed x and y components $w_X, w_Y$ and odor concentration) and the four aforementioned encoded features as inputs. 
% Training and test sets are a randomized non-overlapping 80$\%$--20$\%$ split of evaluation episodes, balanced across plume configuration and episode outcomes.
% We do a 20-trial 3-fold cross-validated randomized search over the number-of-estimators (range: [10,50]) hyperparameter, and then train a classifier using the best hyperparameter on the whole training set. 
% We next estimate the relative importance of each input feature by calculating its permutation importance score \citep{strobl2008conditional, breiman2001random}, which is an estimate of the reduction in the classifierâ€™s accuracy across several (N=30) randomized permutations of that feature.
% Note again that the estimates provided by this analysis are approximate due to the discretization of the action data and correlations between features.


% \begin{table}[h!]
%     \centering
%     \begin{tabular}{ccccc}
%      \hline\hline
%      \textbf{Agent} & \textbf{Agent ID} & \textbf{Test set accuracy} & \textbf{Test set accuracy} & \textbf{Test set accuracy}   \\
%      & & \textbf{(All features)} & \textbf{(Instantaneous only)} & \textbf{(Most freq. class)}   \\
%      \hline   
%         RNN 1 & 2760377 & 0.84 & 0.74 (0.10) & 0.33 (0.51) \\ \hline
%         RNN 2 & 3199993 & 0.67 & 0.49 (0.18) & 0.28 (0.39) \\ \hline  
%         RNN 3 & 3307e9 & 0.82 & 0.69 (0.13) & 0.39 (0.43) \\ \hline
%         RNN 4 & 541058 & 0.70 & 0.53 (0.17) & 0.44 (0.26) \\ \hline
%         RNN 5 & 9781ba & 0.84 & 0.74 (0.10) & 0.40 (0.44) \\ \hline
%      \hline
%     \end{tabular}
%     \caption{Classifier based quantification of contribution of represented features: 
%     In last two columns, quantity in parentheses is the difference in accuracy with respect to classifier that has all features (4 represented features and instantaneous egocentric sensory features). 
%     Represented features contribute to higher test accuracy. 
%     See Section \ref{sec_repr} for further details.
%     }
% \label{table_supp_repr}
% \end{table}


\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.40\linewidth]{comsub_agent_angle_ground_2760377.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_lastenc_2760377.png} \\
\includegraphics[width=0.40\linewidth]{comsub_odor_ewm_8_2760377.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_enc_62_2760377.png} \\
\includegraphics[width=0.30\linewidth]{R2s_common_2760377.png}
\includegraphics[width=0.34\linewidth]{repr_2760377.png}
\includegraphics[width=0.30\linewidth]{scree_2760377.png}
\caption[Neural representations -- Agent 1]{Neural representations -- Agent 1 (See Figure \ref{fig_representations} for equivalent data on Agent 3 and figure details)}
\end{center}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.40\linewidth]{comsub_agent_angle_ground_3199993.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_lastenc_3199993.png} \\
\includegraphics[width=0.40\linewidth]{comsub_odor_ewm_10_3199993.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_enc_44_3199993.png} \\
\includegraphics[width=0.30\linewidth]{R2s_common_3199993.png}
\includegraphics[width=0.34\linewidth]{repr_3199993.png}
\includegraphics[width=0.30\linewidth]{scree_3199993.png}
\caption{Neural representations -- Agent 2}
\end{center}
\end{figure*}


\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.40\linewidth]{comsub_agent_angle_ground_3307e9.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_lastenc_3307e9.png} \\
\includegraphics[width=0.40\linewidth]{comsub_odor_ewm_8_3307e9.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_enc_46_3307e9.png} \\
\includegraphics[width=0.30\linewidth]{R2s_common_3307e9.png}
\includegraphics[width=0.34\linewidth]{repr_3307e9.png}
\includegraphics[width=0.30\linewidth]{scree_3307e9.png}
\caption[Neural representations -- Agent 3]{Neural representations -- Agent 3 (Same agent as in Figure \ref{fig_representations})}
\end{center}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.40\linewidth]{comsub_agent_angle_ground_541058.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_lastenc_541058.png} \\
\includegraphics[width=0.40\linewidth]{comsub_odor_ewm_6_541058.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_enc_40_541058.png} \\
\includegraphics[width=0.30\linewidth]{R2s_common_541058.png}
\includegraphics[width=0.34\linewidth]{repr_541058.png}
\includegraphics[width=0.30\linewidth]{scree_541058.png}
\caption{Neural representations -- Agent 4}
\end{center}
\end{figure*}


\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.40\linewidth]{comsub_agent_angle_ground_9781ba.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_lastenc_9781ba.png} \\
\includegraphics[width=0.40\linewidth]{comsub_odor_ewm_12_9781ba.png}
\includegraphics[width=0.40\linewidth]{comsub_odor_enc_44_9781ba.png} \\
\includegraphics[width=0.30\linewidth]{R2s_common_9781ba.png}
\includegraphics[width=0.34\linewidth]{repr_9781ba.png}
\includegraphics[width=0.30\linewidth]{scree_9781ba.png}
\caption{Neural representations -- Agent 5}
\end{center}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\subsection{Structured neural dynamics}
\label{sec_supp_dynamics}

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
     \hline\hline
     \textbf{Agent} & \textbf{Agent ID} & \textbf{Limit-cycle period}  \\
     \hline   
        RNN 1 & 2760377 & 19 steps (0.76 s) \\ \hline
        RNN 2 & 3199993 & NA (clear periodic structure not observed)  \\ \hline
        RNN 3 & 3307e9 & 17 steps (0.68 s) \\ \hline
        RNN 4 & 541058 & 28 steps (1.12 s) \\ \hline
        RNN 5 & 9781ba & 18 steps (0.72 s) \\ \hline
     \hline
    \end{tabular}
    \caption{Limit cycle periods for each RNN agent}
\label{table_supp_LC}
\end{table}
% (19+17+28+18)/4 = 20.5 steps or 0.82 sec


\begin{figure*}[h!]
\centering
\includegraphics[width=0.40\linewidth]{regime_traj_2760377_HOME.png}
\includegraphics[width=0.20\linewidth]{regime_neural_2760377_HOME.png} \\
\includegraphics[width=0.40\linewidth]{regime_traj_2760377_OOB.png}
\includegraphics[width=0.20\linewidth]{regime_neural_2760377_OOB.png} \\
\includegraphics[width=0.40\linewidth]{comsub_regime_2760377.png}
% \includegraphics[width=0.40\linewidth]{limitcycle_2760377.png}
\caption[Neural dynamics -- Agent 1]{Neural dynamics -- Agent 1 (See Figure \ref{fig_dynamics} for equivalent data on Agent 3 and figure details)}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.40\linewidth]{regime_traj_3199993_HOME.png}
\includegraphics[width=0.20\linewidth]{regime_neural_3199993_HOME.png} \\
\includegraphics[width=0.40\linewidth]{regime_traj_3199993_OOB.png}
\includegraphics[width=0.20\linewidth]{regime_neural_3199993_OOB.png} \\
\includegraphics[width=0.40\linewidth]{comsub_regime_3199993.png}
% \includegraphics[width=0.40\linewidth]{limitcycle_3199993.png}
\caption{Neural dynamics -- Agent 2}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.40\linewidth]{regime_traj_3307e9_HOME.png}
\includegraphics[width=0.20\linewidth]{regime_neural_3307e9_HOME.png} \\
\includegraphics[width=0.40\linewidth]{regime_traj_3307e9_OOB.png}
\includegraphics[width=0.20\linewidth]{regime_neural_3307e9_OOB.png} \\
\includegraphics[width=0.40\linewidth]{comsub_regime_3307e9.png}
% \includegraphics[width=0.40\linewidth]{limitcycle_3307e9.png}
\caption[Neural dynamics -- Agent 3]{Neural dynamics -- Agent 3 (same as Figure \ref{fig_dynamics})}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.40\linewidth]{regime_traj_541058_HOME.png}
\includegraphics[width=0.20\linewidth]{regime_neural_541058_HOME.png} \\
\includegraphics[width=0.40\linewidth]{regime_traj_541058_OOB.png}
\includegraphics[width=0.20\linewidth]{regime_neural_541058_OOB.png} \\
\includegraphics[width=0.40\linewidth]{comsub_regime_541058.png}
% \includegraphics[width=0.40\linewidth]{limitcycle_541058.png}
\caption{Neural dynamics -- Agent 4}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.40\linewidth]{regime_traj_9781ba_HOME.png}
\includegraphics[width=0.20\linewidth]{regime_neural_9781ba_HOME.png} \\
\includegraphics[width=0.40\linewidth]{regime_traj_9781ba_OOB.png}
\includegraphics[width=0.20\linewidth]{regime_neural_9781ba_OOB.png} \\
\includegraphics[width=0.40\linewidth]{comsub_regime_9781ba.png}
% \includegraphics[width=0.40\linewidth]{limitcycle_9781ba.png}
\caption{Neural dynamics -- Agent 5}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
% \clearpage
% \section{Transitions between neural clusters}
% \label{sec_supp_ttcs}

% \textbf{Transition duration calculations:}
% To define entry into a neural activity regime, we first define neural activity `centroids' associated with the \textit{tracking} and \textit{lost} neural activity regimes.
% These are the average of the last 1-second's neural activity from successful evaluation episodes that home in on odor source (HOME), and unsuccessful evaluation episodes where the agent straying Out Of (arena) Bounds (OOB), respectively (see Figure \ref{fig_ttcs}). 
% We then define neural activity clusters associated with the HOME and OOB centroids as being comprised of all neural activity with a distance $D/2$ units from the respective centroid, where $D$ is the distance between centroids.
% Finally,  for any unsuccessful tracking episode, we calculate a `time to \textit{lost}' (TTL) as the duration between the agent leaving the plume and entering the OOB cluster.
% Similarly, for successfully homing episodes, we calculate a `time to \textit{track}' (TTT) as the time taken to enter the HOME cluster after entering the plume.
% In calculating TTT, we exclude small excursions outside the plume where the agent is skimming the boundary of the plume and only consider excursions where the agent has entered the \textit{recovering} or \textit{lost} behavioral module.
% We split TTT into two types, labeling it `time to \textit{track} not \textit{lost}' (TTT-NL) if the agent was in \textit{recovering} or `time to \textit{track} after \textit{lost}' (TTT-L) if the agent was in \textit{lost} before entering the plume. \\

% \textbf{Statistical significance calculations:}
% All plots use the Mann-Whitney-Wilcoxon test two-sided with Bonferroni correction, where p-value annotations indicate:  
% \begin{verbatim}
% ns: 5.00e-02 < p <= 1.00e+00 (not significant)
% *: 1.00e-02 < p <= 5.00e-02
% **: 1.00e-03 < p <= 1.00e-02
% ***: 1.00e-04 < p <= 1.00e-03
% ****: p <= 1.00e-04
% \end{verbatim}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.35\linewidth]{comsub_by_centroid_2760377.png} 
% \includegraphics[width=0.30\linewidth]{ttcs_box_2760377.png}
% % \includegraphics[width=0.30\linewidth]{ttcs_swarm_2760377.png}
% \caption[Transitions between neural activity regimes -- Agent 1]{Transitions between neural activity regimes -- Agent 1 (compare with Agent 3 in Figure \ref{fig_ttcs})
% }
% \end{figure*}



% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.35\linewidth]{comsub_by_centroid_3199993.png} 
% \includegraphics[width=0.30\linewidth]{ttcs_box_3199993.png}
% % \includegraphics[width=0.30\linewidth]{ttcs_swarm_3199993.png}
% \caption[Transitions between neural activity regimes -- Agent 2]{Transitions between neural activity regimes -- Agent 2 (NB: this agent does not follow the trend)}
% \end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.35\linewidth]{comsub_by_centroid_3307e9.png} 
% \includegraphics[width=0.30\linewidth]{ttcs_box_3307e9.png}
% % \includegraphics[width=0.30\linewidth]{ttcs_swarm_3307e9.png}
% \caption[Transitions between neural activity regimes -- Agent 3]{Transitions between neural activity regimes -- Agent 3 (same data as Figure \ref{fig_ttcs})}
% \end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.35\linewidth]{comsub_by_centroid_541058.png} 
% \includegraphics[width=0.30\linewidth]{ttcs_box_541058.png}
% % \includegraphics[width=0.30\linewidth]{ttcs_swarm_541058.png}
% \caption{Transitions between neural activity regimes -- Agent 4}
% \end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.35\linewidth]{comsub_by_centroid_9781ba.png} 
% \includegraphics[width=0.30\linewidth]{ttcs_box_9781ba.png}
% % \includegraphics[width=0.30\linewidth]{ttcs_swarm_9781ba.png}
% \caption{Transitions between neural activity regimes -- Agent 5}
% \end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #############################
\clearpage
\subsection{RNN connectivity and stimulus integration timescales}
\label{sec_supp_eigen}

% \textbf{Stimulus integration timescale $\tau_i$ calculation}:
% (First see background on RNNs provided in Section \ref{sec_eigen}).
% Prior literature has looked at the eigenvalues and eigenvectors of the recurrence Jacobian (and recurrence matrix) to investigate how connectivity affects the dynamics of the network \citep{rajan2006eigenvalue,maheswaranathan2019reverse}.  
% Specifically \citep{maheswaranathan2019reverse} obtains the stimulus integration timescale $\tau_i$ associated with a stable eigenvalue $\lambda_i$ (i.e. $|\lambda_i| \leq 1$), by looking at the discrete-time iteration \mbox{$h_i(t)=\lambda_i^{t} h_i(0)$} that governs the integration of stimulus in the direction of eigenvector $v_i$ associated with $\lambda_i$.
% They then compare this with the equivalent continuous time equation 
% \mbox{$h_i(t)=h_i(0) e^{-t / \tau_i}$}, to get 
% \mbox{$\tau_i = \left| ( 1/ \ln|\lambda_i| ) \right|$}.



\begin{figure*}[h!]
\centering
\includegraphics[width=0.45\linewidth]{eigenspectra_2760377.png}
\includegraphics[width=0.27\linewidth]{timescales_2760377.png}
% \includegraphics[width=0.25\linewidth]{eigpower_2760377.png}
\caption[Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 1]{Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 1 (compare with Agent 3 in Figure \ref{fig_eigen_mlps})}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.45\linewidth]{eigenspectra_3199993.png}
\includegraphics[width=0.27\linewidth]{timescales_3199993.png}
% \includegraphics[width=0.25\linewidth]{eigpower_3199993.png}
\caption{Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 2}
\end{figure*}


\begin{figure*}[h!]
\centering
\includegraphics[width=0.45\linewidth]{eigenspectra_3307e9.png}
\includegraphics[width=0.27\linewidth]{timescales_3307e9.png}
% \includegraphics[width=0.25\linewidth]{eigpower_3307e9.png}
\caption[Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 3]{Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 3 (same as Figure \ref{fig_eigen_mlps})}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.45\linewidth]{eigenspectra_541058.png}
\includegraphics[width=0.27\linewidth]{timescales_541058.png}
% \includegraphics[width=0.25\linewidth]{eigpower_541058.png}
\caption{Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 4}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.45\linewidth]{eigenspectra_9781ba.png}
\includegraphics[width=0.27\linewidth]{timescales_9781ba.png}
% \includegraphics[width=0.25\linewidth]{eigpower_9781ba.png}
\caption{Eigenspectra of $\bW_h$ before and after training, and stimulus integration timescales -- Agent 5}
\end{figure*}



%%%%%%%%%
\clearpage
\subsection{Additional figures for reduced (0.5x) radial diffusion-rate plumes}
\label{sec_supp_rediff}
% Latex Layout for SI figures -- 0.5x version of Fig 2

\noindent The four figures below are similar to Figure \ref{fig_behavior_qual} in the main manuscript, except for that all plume simulations use a reduced (0.5x) radial diffusion-rate to encourage highly intermittent odor encounters.
Trajectories have been chosen from successful episodes of RNN Agent 3 across four plume configurations.
As can be seen, trained agents successfully localize the odor source with these more intermittent plumes and tracking behaviors are qualitatively no different than those seen on higher puff-density (higher radial diffusion-rate) plumes. 

% Constant wind direction
\begin{figure*}[h!]
\centering
\begin{subfigure}{0.44\textwidth}
    \includegraphics[width=\textwidth]{output_16_1}
    \caption{Agent trajectory}
\end{subfigure}%
\vspace{0.00mm} 
\begin{subfigure}{0.14\textwidth}
    \includegraphics[width=\textwidth,angle=90,origin=c]{output_16_3.png}
    \caption{Vertical odor profiles}
\end{subfigure}%
\begin{subfigure}{0.40\textwidth}
    \includegraphics[width=\textwidth]{output_16_6.png} 
    \caption{Odor sensed over trajectory} \\
    \includegraphics[width=\textwidth]{output_16_4.png}
    \caption{Horizontal odor profile}
\end{subfigure}%
\caption{Constant wind direction}
% Wind direction switches once
\begin{subfigure}{0.40\textwidth}
    \includegraphics[width=\textwidth]{output_16_8.png}
    \caption{Agent trajectory}
\end{subfigure}%
\begin{subfigure}{0.18\textwidth}
    \includegraphics[width=\textwidth,angle=90,origin=c]{output_16_10.png}
    \caption{Vertical odor profiles}
\end{subfigure}%
\begin{subfigure}{0.40\textwidth}
    \includegraphics[width=\textwidth]{output_16_13.png} 
    \caption{Odor sensed over trajectory} \\
    \includegraphics[width=\textwidth]{output_16_11.png}
    \caption{Horizontal odor profile}
\end{subfigure}%
\caption{Wind direction switches once}
% Wind direction switches many times
\begin{subfigure}{0.38\textwidth}
    \centering
    \includegraphics[width=\textwidth]{output_16_15.png}
    \caption{Agent trajectory}
\end{subfigure}%
\begin{subfigure}{0.20\textwidth}
    \centering
    \includegraphics[width=\textwidth,angle=90,origin=c]{output_16_17.png}
    \caption{Vertical odor profiles}
\end{subfigure}%
\begin{subfigure}{0.40\textwidth}
    \includegraphics[width=\textwidth]{output_16_20.png} 
    \caption{Odor sensed over trajectory} \\
    \includegraphics[width=\textwidth]{output_16_18.png}
    \caption{Horizontal odor profile}
\end{subfigure}%
\caption{Wind direction switches many times}
% Sparser plume
\begin{subfigure}{0.43\textwidth}
    \centering
    \includegraphics[width=\textwidth]{output_16_22}
    \caption{Agent trajectory}
\end{subfigure}%
\begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=\textwidth,angle=90,origin=c]{output_16_24.png}
    \caption{Vertical odor profiles}
    \label{fig:second}
\end{subfigure}%
\begin{subfigure}{0.40\textwidth}
    \includegraphics[width=\textwidth]{output_16_27.png} 
    \caption{Odor sensed over trajectory} \\
    \includegraphics[width=\textwidth]{output_16_25.png}
    \caption{Horizontal odor profile}
\end{subfigure}%
\caption{Sparser plume (0.4x birthrate, 0.5x radial diffusion-rate) and constant wind direction (Same as Figure \ref{fig_behavior_qual}c) 
}
\end{figure*}

%%%%%%% 0.5x reduced diffusion-rate plots %%%%%%%
% \clearpage
% \subsection{Additional figures for reduced (0.5x) diffusion-rate plumes [Agent 3]}
% \label{sec_supp_reduceddiff}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.99\linewidth]{fig_regimes_coursedir_d50.pdf}
\caption{Figure similar to Figure \ref{fig_regimes_coursedir} in the main manuscript, except for that all plume simulations use a reduced (0.5x) radial diffusion-rate to encourage highly intermittent odor encounters. 
We see no qualitative differences with respect to higher puff-density (higher radial diffusion-rate) plumes.
All plots are for RNN Agent 3.
}
\end{figure*}



\begin{figure*}[h!]
\centering
\includegraphics[width=0.99\linewidth]{fig_repr_d50.pdf}
\caption{
Figure similar to Figure \ref{fig_representations} in the main manuscript, except for that all plume simulations use a reduced (0.5x) radial diffusion-rate to encourage highly intermittent odor encounters. 
We see a qualitative difference in PCA plots of the neural dynamics, in that they now seem to highlight dynamics associated with the `recover' regime (and to some extent, the `lost' regime)  more. 
We believe this is because the underlying trajectories indeed have more recover/lost regime type behaviors due to increased intermittency of odor encounters.
No qualitative differences with respect to Figure \ref{fig_representations} are seen in regards to behaviorally relevant quantities represented.
All plots are for RNN Agent 3.
}
\label{fig_supp_repr_d50}
\end{figure*}



\begin{figure*}[h!]
\centering
\includegraphics[width=0.99\linewidth]{fig_dynamics_d50.pdf}
\caption{
Figure similar to Figure \ref{fig_dynamics} in the main manuscript, except for that all plume simulations use a reduced (0.5x) radial diffusion-rate to encourage highly intermittent odor encounters. 
We see no qualitative differences with respect to Figure \ref{fig_representations} in either behaviors or their representations.
Neural activity PCA plot highlights dynamics associated with the `recover' and `lost' behaviors due to distribution shift as also seen in Figure \ref{fig_supp_repr_d50}.
All plots are for RNN Agent 3.
}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.99\linewidth]{home_by_arch_facet_d50.png}
\caption{
Figure similar to Figure \ref{fig_eigen_mlps} in the main manuscript, except for that all plume simulations use a reduced (0.5x) radial diffusion-rate to encourage highly intermittent odor encounters.
For fairness of comparison with Figure \ref{fig_eigen_mlps}, we have run our behavior assay on all 14 trained agents of each architecture type (RNNs and MLPs) using reduced (0.5x) radial diffusion-rate plumes. 
We then re-selected the top-5 performing agents for each architecture type to generate the above figure (see Methods for behavior assay and agent selection details). 
We observe that 2 out of the top-5 RNNs used in Figure \ref{fig_eigen_mlps} are different from those selected here (no comparable bookkeeping was done for the MLPs).
We roughly see the same trends in this plot as in Figure \ref{fig_eigen_mlps}, in that RNNs clearly outperform MLPs across plume configurations.
Trends across MLPs show lesser statistical significance, possibly because MLP training produced agents that are less robust to domain shift (reduced radial diffusion rates) than RNNs.
Future work could explore retraining all agents across all architectures on plume with reduced (0.5x) puff radial diffusion-rates for better generalization.
}
\label{sec_supp_reduceddiff_fig6}
\end{figure*}


\end{document}
